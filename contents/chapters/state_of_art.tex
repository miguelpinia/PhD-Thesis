\chapter{\label{chapter:2_State_of_art}State of the Art}

From the end of World War II until the 1990s, most computing was conducted on single-core processors. Operating systems utilized schedulers and other techniques to simulate concurrency. In 2001, IBM created the first multi-core processor~\cite{ibmIBM100Power}. This innovation enables two processors to collaborate and operate at high bandwidths (for its time), benefiting from significant on-chip memories and high-speed buses. Over time, processor manufacturers equip processors with more cores. It's essential to remember Moore's law, which suggests that the number of transistors in the same space continues to increase yearly. However, this results in smaller electronic components and circuits, which cannot be made faster without overheating. As a result, many industries are adopting ``multi-core'' architectures. Several processors communicate through shared memory in this setup, using hardware caches and RAM. This allows for more effective computing by utilizing parallelism, where the processors work together on a single task~\cite{DBLP_books_daglib_0020056}.

Implementing multiprocessors has brought about a significant shift in our software development approach. We can run multiple tasks in parallel in these multiprocessors and build more sophisticated tasks that use multiprocessors in parallel, executing several sub-tasks. We must exploit this parallelism to boost our efficiency. Usually, we can use techniques from parallel, distributed, or concurrent computing, which are related but distinct fields. Programming for multiprocessors can be complex due to the inherent asynchronicity of modern computer systems. This project centers on concurrent computing and explores the shift from traditional to more flexible approaches (relaxed concurrent computing). It takes a theoretical approach but with practical applications in mind.

\section{\label{section:classic-concurrent}Classic Concurrent Computing}

In the past, sequential computing was the standard method for performing computations before the emergency of concurrent computing. Based on the contributions of  Turing~\cite{DBLP_journals_x_Turing37} and Von Neumann~\cite{DBLP_journals_annals_Neumann93}, sequential computing involves executing instructions one after the other using a processor. This can be represented by the analogy of \textit{objects} being modified by a \textit{process} through \textit{atomic operations}. The relationship between operations and objects can be defined in terms of \textit{preconditions} and \textit{postconditions}, which define the state of an object before and after applying the operation\footnote{Since now, we will refer ``\textit{operation}'' as ``\textit{method}'' as in the context of \textit{Object-Oriented Programming}.}. This is similar to an API documentation\footnote{(Application Program Interface)}, which usually explains the object's state before invoking a method of the object and the result of calling the operation, which can be a particular value or throw an exception. This style of documentation is known as ``\textit{sequential specification}''.
However, expressing the relationship between objects and methods fails when several processes share such objects. The problem is that if many processes can invoke an object's operation concurrently, what invocation is first? What is the state after the execution of these overlapping invocations? Does it make sense to talk about operation order?

In concurrent systems, three consistency models are usually utilized as a correctness condition: \textit{Serializability}, \textit{Sequential Consistency}, and \textit{Linearizability}. The concept of Serializability was initially explained by Papadimitriou~\cite{DBLP_journals_jacm_Papadimitriou79b}. Lamport introduced the notion of Sequential Consistency~\cite{lamport1979how}. Herlihy and Wing introduced the idea of Linearizability~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}.
Serializability in concurrent computing guarantees the correctness and isolation of transactions in a multi-user database or concurrent system. It ensures that when executing a set of transactions concurrently, the final result is equivalent to executing them one after another without overlap, mimicking a serial execution order. This helps maintain consistency and prevents errors in the system. 
Sequential Consistency requires shared variable operations in concurrent systems to appear executed sequentially according to program order. Linearizability is a stricter condition that guarantees Sequential Consistency and ensures that the global order of operations includes a specific point in time (i.e., linearization point) for each operation. This ensures that each operation appears to take effect atomically at some point between its invocation and response. Linearizability refines the concept of Sequential Consistency by imposing a stricter requirement on the sequence of methods. This ensures that the system's observed behavior aligns with a valid sequential execution of the methods. Therefore, while Sequential Consistency allows for multiple valid orders of operations as long as they respect program order, Linearizability enforces a stricter condition by requiring operations to appear as if they occurred instantaneously at some specific point between invocation and response. 

In a concurrent multi-process system, a progress condition outlines the assurance of process progress. It sets specific requirements that ensure processes in the system will keep advancing toward completing their tasks. Progress conditions are partitioned into \textit{blocking} and \textit{non-blocking}. Two blocking progress conditions rely on lock-based synchronization: Deadlock-freedom and starvation-freedom~\cite{DBLP_books_daglib_0020056}. \textit{Deadlock-freedom} guarantees that processes will not deadlock and at least one process will make progress; this means that a process acquiring a lock will release it; in other words, a process trying to acquire the lock eventually succeeds. \textit{Starvation-freedom} ensures that every thread progresses as long as no other thread holds the lock.

On the other hand, there are three \textit{non-blocking} progress conditions: \textit{Obstruction-Free}~\cite{DBLP_conf_icdcs_HerlihyLM03}, \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90} and \textit{Wait-Free}~\cite{DBLP_journals_toplas_Herlihy91}. 
\textit{Lock-free} progress condition ensures that \textit{some} method invocation finishes in a finite number of steps. \textit{Wait-free} progress condition~\cite{DBLP_journals_toplas_Herlihy91} is stronger than lock-free, where \textit{every} method invocation finishes its execution in a finite number of steps. When using lock-free methods, the system as a whole will make progress, but it does not guarantee that any specific thread will make progress. This is because lock freedom ensures \textit{minimal progress}. On the other hand, wait-freedom ensures the \textit{maximal progress}: any process that continues to take steps will make progress. Obstruction-free~\cite{DBLP_conf_icdcs_HerlihyLM03} only guarantees progress if no other processes actively interfere with the process making progress. \textit{Sequential Consistency} and \textit{Linearizability} are \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}.

Consistency models and progress conditions are properties of the concurrent objects that show how they should behave and how they make progress. However, we still need other properties that tell us how powerful the methods are for solving synchronization problems. Herlihy introduced the notion of \textit{consensus number}~\cite{DBLP_journals_toplas_Herlihy91} as a measure of the computational power of concurrent objects. The \textit{consensus number} of a concurrent object is the maximum number of processes that can solve an elementary synchronization problem known as \textit{consensus} using concurrent objects, which are often called \textit{synchronization primitives}. Herlihy demonstrates that there exists an infinite hierarchy of synchronization primitives. No primitive at any given level can be used for a wait-free or lock-free implementation of any primitive at higher levels~\cite{DBLP_journals_toplas_Herlihy91}, i.e., in a system of \(n\) or more concurrent processes, it is impossible to implement a wait-free or lock-free object with consensus number \(n\) using objects with a lower consensus number.

Implementing efficient and correct concurrent algorithms is known to be a difficult problem.
To address the problem, currently, multiprocessors provide synchronization instructions that can be expressed as \textit{Read-Modify-Write} (RMW) operations\footnote{e.g., Compare\&Swap or Test\&Set.}, with high coordination power (measured through the consensus number~\cite{DBLP_journals_toplas_Herlihy91}), which are in principle slower than simple \textit{Read}/\textit{Write} instructions\footnote{In practice, an uncontended \textit{Read-Modify-Write} instruction can be faster than contended \textit{Read}/\textit{Write} instructions due to contention.}. In addition, certain programs may utilize Read-After-Write synchronization patterns that rely on the flag principle (see, for example,~\cite {DBLP_books_daglib_0020056}). This involves writing to a shared variable and then reading another variable. To ensure proper implementation of this synchronization pattern on multi-core architectures, a \textit{memory fence} (referred to as a \textit{barrier}) should be explicitly added to prevent the compiler or architecture from rearranging Read and Write instructions. It has been demonstrated that building concurrent implementations of classic and ubiquitous specifications\footnote{Such as sets, queues, stacks, and mutual exclusion.} in the standard asynchronous shared memory model must use \textit{Read-After-Write} synchronization patterns or atomic \textit{Read-Modify-Write} instructions. Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} addresses the fundamental limitation in concurrent algorithms, arguing that the necessity of synchronization mechanisms is intrinsic and cannot be eliminated without incurring significant costs.

\section{\label{section:relaxed-concurrent}Relaxed Concurrent Computing}



\section{\label{section:work-stealing}Work-Stealing}
\section{\label{section:data-structures}Data-Structures}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% TeX-parse-self: t
%%% TeX-auto-save: t
%%% End:
