\chapter{\label{chapter:2_State_of_art}State of the Art}

From the end of World War II until the 1990s, most computers had only single processor core.  To simulate concurrency, operating systems used schedulers and other techniques. In 2001, IBM created the first multicore processor, which enabled two processors to work together at high bandwidths and benefit from significant on-chip memories and high-speed buses. As time went on, processors were equipped with more cores~\cite{ibmIBM100Power}. It's important to remember Moore's Law, which states that the number of transistors in the same space keeps increasing every year. However, this results in smaller electronic components and circuits, which cannot be made faster without overheating. As a result, many industries are now using ``multicore'' architectures. In this setup, several processors communicate through shared memory, using hardware caches and RAM. This allows more effective computing through parallelism, where the processors work together on a single task\cite{DBLP_books_daglib_0020056}.

The advent of multiprocessors has revolutionized the way we approach software development. By exploiting parallelism, we can run complex algorithms faster by dividing them into smaller sub-tasks. This can be achieved using techniques from parallel, distributed, or concurrent computing. However, programming multiprocessors can be challenging because modern computer systems are inherently asynchronous.

This thesis explores the shift from traditional to more flexible approaches in concurrent computing to programming concurrent algorithms. This takes a theoretical approach but with practical applications in mind.

\section{\label{section:classic-concurrent}Classic Concurrent Computing}

Sequential computing has been the standard method of performing computations since the early days of electronic computing, before the advent of concurrent, parallel, and distributed computing. Sequential computing involves executing instructions one after the other using a processor, based on the contributions of Turing~\cite{DBLP_journals_x_Turing37} and Von Neumann~\cite{DBLP_journals_annals_Neumann93}. In this way, \textit{processes} modify \textit{objects} through \textit{atomic operations}, where the relationship between operations and objects can be defined in terms of \textit{preconditions} and \textit{postconditions}. This is similar to an API documentation, in which describes the state of an object before and after a method\footnote{Since now, we will refer to ``\textit{operation}'' as ``\textit{method}'' like is used in the context of \textit{Object-Oriented Programming}.} is called on the object, as well as the method's output, which can be a specific value or throw an exception. This style of documentation is known as ``sequential specification''.

However, this way of expressing the relationship between objects and methods falls short when several processes share such objects. This is because if many processes can invoke an object's operation concurrently,  what invocation is first? What is the state after the execution of these overlapping invocations? Does it make sense to talk about operation order?

In concurrent systems, three consistency models are usually utilized as a correctness condition: \textit{Serializability}, \textit{Sequential Consistency}, and \textit{Linearizability}. The concept of Serializability was initially explained by Papadimitriou~\cite{DBLP_journals_jacm_Papadimitriou79b}. Lamport introduced the notion of Sequential Consistency~\cite{lamport1979how}. Herlihy and Wing introduced the idea of Linearizability~\cite{DBLP_journals_toplas_HerlihyW90}.
Serializability in concurrent computing guarantees the correctness and isolation of transactions in a multi-user database or concurrent system. It ensures that when executing a set of transactions concurrently, the final result is equivalent to running them one after another without overlap, mimicking a serial execution order. This helps maintain consistency and prevents errors in the system.
Sequential consistency requires shared variable operations in concurrent systems to appear executed sequentially according to program order. Linearizability is a stricter condition that guarantees Sequential Consistency and ensures that the global order of operations includes a specific point in time (i.e., linearization point) for each operation. This ensures that each operation appears to take effect atomically at some point between its invocation and response. Linearizability refines the concept of Sequential Consistency by imposing a stricter requirement on the sequence of methods. This ensures that the system's observed behavior aligns with a valid sequential execution of the methods. Therefore, while Sequential Consistency allows for multiple valid orders of operations as long as they respect program order, Linearizability enforces a stricter condition by requiring operations to appear as if they occurred instantaneously at some specific point between invocation and response.

In a concurrent multi-process system, a progress condition outlines the assurance of process progress. It sets specific requirements that ensure processes in the system will keep advancing toward completing their tasks. Progress conditions are partitioned into \textit{blocking} and \textit{non-blocking}. Two blocking progress conditions rely on lock-based synchronization: Deadlock-freedom and starvation-freedom~\cite{DBLP_books_daglib_0020056}. \textit{Deadlock-freedom} guarantees that processes will not deadlock and at least one process will make progress; this means that a process acquiring a lock will release it; in other words, a process trying to acquire the lock eventually succeeds. \textit{Starvation-freedom} ensures that every thread progresses as long as no other thread holds the lock.

On the other hand, there are three \textit{non-blocking} progress conditions: \textit{Obstruction-Free}~\cite{DBLP_conf_icdcs_HerlihyLM03}, \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90} and \textit{Wait-Free}~\cite{DBLP_journals_toplas_Herlihy91}.
\textit{Lock-free} progress condition ensures that \textit{some} method invocation finishes in a finite number of steps. \textit{Wait-free} progress condition~\cite{DBLP_journals_toplas_Herlihy91} is stronger than lock-free, where \textit{every} method invocation finishes its execution in a finite number of steps. When using lock-free methods, the system as a whole will make progress, but it does not guarantee that any specific thread will make progress. This is because lock freedom ensures \textit{minimal progress}. On the other hand, wait-freedom ensures the \textit{maximal progress}: any process that continues to take steps will make progress. Obstruction-free~\cite{DBLP_conf_icdcs_HerlihyLM03} only guarantees progress only if no other processes actively interfere with the process making progress. This makes the condition strictly weaker than \textit{lock-free}. %\textit{Sequential Consistency} and \textit{Linearizability} are \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90}.

Consistency models and progress conditions are properties of the concurrent objects that show how they should behave and how they make progress. However, we still need other properties that tell us how powerful the methods are for solving synchronization problems. Herlihy introduced the notion of \textit{consensus number}~\cite{DBLP_journals_toplas_Herlihy91} as a measure of the computational power of concurrent objects. The \textit{consensus number} of a concurrent object is the maximum number of processes that can solve an elementary synchronization problem known as \textit{consensus} using concurrent objects, which are often called \textit{synchronization primitives}. Herlihy demonstrates that an infinite hierarchy of synchronization primitives exists. No primitive at any given level can be used for a wait-free or lock-free implementation of any primitive at higher levels~\cite{DBLP_journals_toplas_Herlihy91}, i.e., in a system of \(n\) or more concurrent processes, it is impossible to implement a wait-free or lock-free object with consensus number \(n\) using objects with a lower consensus number.

Programming efficient and correct concurrent algorithms is known to be a complex problem. To address the problem, currently, multiprocessors provide synchronization instructions that can be expressed as \RMW (RMW) operations\footnote{e.g., \CAS or \TAS}, with high coordination power (measured through the consensus number~\cite{DBLP_journals_toplas_Herlihy91}), which are in principle slower than simple \R/\W instructions\footnote{In practice, an uncontended \RMW instruction can be faster than contended \R/\W instructions due to contention.}.

Additionaly, some programs may utilize \RAW synchronization patterns that rely on the flag principle (see, for example,~\cite {DBLP_books_daglib_0020056}). This involves writing to a shared variable and then reading another variable. To ensure proper implementation of this synchronization pattern on multicore architectures, a \textit{memory fence} (also known as a \textit{barrier}) should be explicitly added to prevent the compiler or architecture from rearranging \R and \W instructions.

It has been demonstrated that building concurrent implementations of classic and ubiquitous specifications\footnote{Such as sets, queues, stacks, and mutual exclusion.} in the standard asynchronous shared memory model must use \RAW synchronization patterns or atomic \RMW instructions. Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} addresses the fundamental limitation in concurrent algorithms, arguing that the necessity of synchronization mechanisms is intrinsic and cannot be eliminated without incurring significant costs. Ellen et al.~\cite{DBLP_journals_siamcomp_EllenHS12} show that shared data structures are often inherently sequential and cannot be easily parallelized. Attiya et al.~\cite{DBLP_journals_jacm_AttiyaGHK09} explores the advantages and drawbacks of obstruction-free implementations over other synchronization methods. These implementations can avoid the scalability and fault-tolerance problems that arise from traditional locking-based techniques, which can become a bottleneck in highly concurrent systems. Obstruction-free implementations can perform well without step contention but have high worst-case complexity.

\section{\label{section:relaxed-concurrent}Relaxed Concurrent Computing}

The work of Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} has shown that it is impossible to eliminate expensive synchronization in classic and ubiquitous specifications. This leads us to question \textit{if it is possible to bypass this impossibility result in any way}. There are two possible ways to circuvement this result: (1) consider relaxed semantics for the algorithms and (2) make additional assumptions about the model.

Software development has become more challenging with the widespread adoption of multicore processors as the standard computing platform. It is critical to optimize the use of all available computer resources, including multiple cores, memory, and storage, for efficient performance. Most programs need data structures, and with all these new multicore computing platforms, concurrent data structures are required for implementing distributed, parallel, and concurrent programs. Designing concurrent data structures is a challenging task. The challenge arises when trying to enhance performance while maintaining correctness. As we strive to improve performance, ensuring the algorithm's correctness becomes increasingly more complex~\cite{DBLP_journals_cacm_Shavit11}. It has been mentioned to improve scalability, traditional data structures must be relaxed. This often involves relaxing correctness and progress conditions\footnote{Often called \textit{safety} and \textit{liveness} respectively.}. By relaxing the ordering guarantees of queues and stacks, performance and scalability can be significantly increased. There are many examples of natural relaxations that demonstrate this~\cite{DBLP_journals_cacm_Shavit11}. In the work of Shavit and Taubenfeld~\cite{DBLP_journals_dc_ShavitT16}, it is pointed out that relaxing the semantics of traditional data structures might be beneficial to reduce synchronization requirements and improve scalability: ``There is a trade-off between synchronization and the ability of an implementation to scale performance with the number of processors. Amdahl’s law implies that even a small fraction of inherently sequential code limits scaling. Using semantically weaker data structures may help reduce the synchronization requirements and improve multicore systems' scalability.''
Two types of relaxation are used: (1) relaxing the sequential specification of traditional data structures and (2) relaxing the requirements for correctness conditions.

An example of the first case is the \(k-FIFO\) queue presented in the work of Kirsch et al. ~\cite{DBLP_conf_pact_KirschLP13, DBLP_conf_ica3pp_KirschPRS12}, where the sequential specification requirement was relaxed. The elements of this queue can be dequeued out-of-order up to a constant \(k \ge 0\) (called k-Out-of-Order). Hezinger et al.~\cite{DBLP_conf_popl_HenzingerKPSS13} address such redefinition of data structure semantics. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to sequential specification: the k-relaxed sequential contains all sequences over the alphabet within distance \(k\) from the original specification. This semantic specification defines the distance in terms of data structures~\cite{DBLP_conf_popl_HenzingerKPSS13}. Shavit and Taubenfeld conducted a theoretical analysis of relaxed queues, stacks, and multisets. They examine whether the relaxation of these data structures' semantics can result in more simple and scalable implementations. The authors evaluate these relaxations from a perspective of computability~\cite{DBLP_journals_dc_ShavitT16}. Also, in the work of Henzinger et al. ~\cite{DBLP_conf_popl_HenzingerKPSS13}, in addition to the definition of K-Out-of-Order, they define the K-Stuttering and the K-Lateness. Concurrent data structures can employ a relaxation scheme known as K-Stuttering, which allows for a certain amount of repetition or stuttering in operation execution. This relaxation enables an operation to be repeated up to \(k\) times before being considered a failure. While this can increase the performance of the data structure by reducing the need for synchronization, it may also compromise its correctness to some extent. The concept of K-lateness involves measuring the duration that an item remains on a stack without removal. This metric is determined by counting the number of pop operations that have occurred (\(k\)) since the item was last the youngest element added to the stack. K-lateness is particularly useful in the context of k-stuttering relaxation for concurrent data structures, as it helps to identify which items can be removed from the stack without violating the relaxation constraints.

In the second case, based on the research conducted by Afek et al.~\cite{DBLP_conf_opodis_AfekKY10}, the concept of quasi-linearizability is a way to quantify limited non-determinism. An object implementation is considered quasi-linearizable if each execution is at a bounded ``distance'' from some linear execution of the object. This definition is more flexible than linearizability and can improve the performance and scalability of concurrent object implementations. To illustrate, quasi-linearizability can be seen as a middle ground between linearizability and weaker consistency models. A quantitative relaxation framework to formally specify relaxed objects is introduced in the work of Henzinger et al.~\cite{DBLP_conf_popl_HenzingerKPSS13}, and this formalism is applied in the work of Haas et al.~\cite{DBLP_conf_cf_HaasLHPSKS13}, where their relaxed queue implementations are instances of a distributed queue, consisting of multiple FIFO queues k-relaxed. In the work of Talmage and Welch~\cite{DBLP_journals_algorithms_TalmageW18}, it is shown that Linearizability and the three data type relaxations (k-Out-of-Order, k-Lateness, and k-Stuttering) studied in~\cite{DBLP_conf_popl_HenzingerKPSS13},  can also be defined as consistency conditions. In~\cite{DBLP_conf_concur_HaasHHKLPSSV16}, the concept of \textit{local linearizability} is introduced. This relaxed consistency condition applies to container data structures such as pools, queues, and stacks. The concept of distributional linearizability, introduced in~\cite{DBLP_conf_spaa_Alistarh0KLN18}, is used to analyze randomized relaxations. This formalism is applied to MultiQueues~\cite{DBLP_conf_spaa_RihaniSD15}, a family of concurrent data structures that implement relaxed concurrent priority queues.

Castañeda, Rajsbaum, and Raynal introduce the concept of \emph{multiplicity}~\cite{DBLP_journals_dc_CastanedaRR23,DBLP_conf_opodis_CastanedaRR20}, which refers to the property of a relaxed queue or stack that allows an item to be returned more than once by different operations, but only in case of concurrency. The property of \emph{multiplicity} will be utilized for relaxation in most of the research presented here.

\section{\label{section:work-stealing}Work-Stealing}

In this work, we are interested in studying how relaxation can be applied to practical environments. In particular, we are interested in applying to work-stealing and data-structures. \emph{Work-stealing} is a popular technique for efficient task parallelization of irregular workloads by implementing dynamic \emph{load balancing}. It has been utilized in various contexts, such as programming languages, parallel-programming frameworks, SAT solvers, and state-space exploration in model checking (e.g.~\cite{DBLP_journals_tpds_AyguadeCDHLMTUZ09,
  DBLP_journals_jpdc_BlumofeJKLRZ96, DBLP_journals_tpds_AyguadeCDHLMTUZ09, DBLP_conf_jvm_FloodDSZ01, DBLP_conf_pldi_FrigoLR98, DBLP_conf_java_Lea00, DBLP_conf_hpca_RangerRPBK07}).

In the work-stealing technique, each process has a set of tasks it needs to complete. The process that owns the task set can put or take tasks from it to complete them. Once a process completes all its tasks (that is, the set is empty), it becomes a \emph{thief} and can \emph{steal} tasks from another process, which is called the \emph{victim}. A work-stealing algorithm offers three main operations: \Put{} and \Take, which are exclusively for the owner's use, and \Steal, which is solely for the thief's use. To guarantee correctness, \emph{Linearizability} condition~\cite{DBLP_journals_toplas_HerlihyW90}  is generally assumed, while \emph{lock-freedom}~\cite{DBLP_journals_toplas_HerlihyW90}  and \emph{wait-freedom}~\cite{DBLP_journals_toplas_Herlihy91} are the typical progress conditions. When designing work-stealing algorithms, the main objective is to ensure that the \emph{Put} and \emph{Take} operations are efficient and easy to use since these are the most frequently used operations by the owner. Unfortunately, it has been demonstrated that any work-stealing algorithm in the standard asynchronous shared memory model must rely on either \RAW{} synchronization patterns or \emph{\RMW} instructions (such as \CAS{} or \TAS)~\cite{DBLP_conf_popl_AttiyaGHKMV11}. The \RAW{} synchronization pattern is based on the \emph{flag principle}, which entails writing on a shared variable and reading another variable (as shown in~\cite{DBLP_books_daglib_0020056}).

To properly implement an algorithm on multicore architectures using a synchronization pattern, it is crucial to include a \emph{memory fence} (also called \emph{barrier}) to prevent the reordering of \R or \W instructions by the compiler or the architecture. However, these fences can be costly and atomic \RMW{} instructions, with high coordination power (which can be formally measured through the \emph{consensus number} formalism~\cite{DBLP_journals_toplas_Herlihy91}), are slower than simple \R/\W instructions\footnote{In practice, contention might be the dominant factor, namely, an uncontended \RMW{} instruction can be faster than contended \R/\W instructions.}. \Take/\Steal{} operations in work-stealing algorithms are based on the flag principle, as found in the literature~\cite{circular.work.stealing, DBLP_conf_pldi_FrigoLR98, non.blocking.work.stealing, 10.1145.571825.571876}. To overcome the impossibility result in~\cite{DBLP_conf_popl_AttiyaGHKMV11}, we must consider work-stealing with relaxed semantics or make additional assumptions on the model. Only a few works, such as~\cite{maged.vechev.2009} and~\cite{fencefreework}, have explored these directions.

Observing that in some contexts, it is ensured that no task is repeated (e.g., by checking first if a task is completed) or the nature of the problem solved tolerates repeatable work (e.g., parallel SAT solvers), Michael, Vechev, and Saraswat propose the concept of \emph{idempotent} work-stealing~\cite{maged.vechev.2009}. This relaxation permits a task to be taken \emph{at least once} instead of \emph{exactly once}. Three idempotent work-stealing algorithms are presented in their paper~\cite{maged.vechev.2009},  where tasks are inserted and extracted in different orders. The relaxation allows each of the algorithms to overcome the impossibility result in~\cite{DBLP_conf_popl_AttiyaGHKMV11} in its \Put{} and \Take{} operations as they use only \R/\W instructions and do not require \RAW{} synchronization patterns. However, the \Steal{} operation uses \CAS, and \Put{} requires that certain \W instructions not be reordered, while \Steal{} needs certain \R instructions not to be reordered either. Thus, fences are required when the algorithms are implemented. Fences between \R (respective \W) instructions are typically not overly costly in practice. As for progress guarantees, \Put{} and \Take{} are wait-free, while \Steal{} is only non-blocking.

Morrison and Afek propose two work-stealing algorithms in~\cite{fencefreeworkproceedings} based on the TSO (Total Store Order) model~\cite{DBLP_journals_cacm_SewellSONM10}. Their \Put{} operation is wait-free and uses only \R/\W instructions, while \Take{} and \Steal{} are either non-blocking and use \CAS, or blocking and use a \emph{lock}. Two well-known algorithms, Cilk THE and Chase-Lev work-stealing, have been adapted here. These adaptations have been modified to work with the TSO model, which prohibits the reordering of \W and \R instructions, eliminating the need for fences between them~\cite{circular.work.stealing, DBLP_conf_pldi_FrigoLR98}. In Morrison and Afek's algorithms, each process has a local buffer for storing \W instructions until they are sent in a FIFO order to the main memory. Their correctness can be affected by reordering \W or \R instructions, but TSO prevents this. To avoid \RAW{} patterns, they assume that the \W buffers have limited size.


\section{\label{section:FIFO-queues}FIFO Queues}

These shared data structures are fundamental and used in all sorts of systems. Shared-memory implementations of concurrent queues have been proposed for more than three decades. Unfortunately, even state-of-the-art concurrent queues experience poor scalability because of high contention arising from \RMW{} instructions such as \CAS{} instruction or the (\FAI) instruction, which manipulate the head and tail of the queue~\cite{DBLP_conf_spaa_FatourouK11, DBLP_conf_ppopp_FatourouK12, basketqueue2007, DBLP_conf_ppopp_KoganP11, DBLP_journals_dc_Ladan-MozesS08, DBLP_conf_podc_MichaelS96, DBLP_journals_topc_Milman-SelaKLLP22, DBLP_conf_ppopp_YangM16}. The latency of RMW instructions increases linearly with the number of contending cores as each instruction acquires exclusive ownership of its cache line.

One of the most popular ways to implement a queue is by utilizing the meaning behind the \FAI{} instruction, which does not fail and always makes progress~\cite{ppopp2013x86queues, DBLP_conf_ppopp_YangM16}. In many queue implementations, a queue operation retries a failed \CAS{} until it succeeds~\cite{DBLP_conf_spaa_FatourouK11, DBLP_conf_ppopp_FatourouK12, basketqueue2007, DBLP_conf_ppopp_KoganP11, DBLP_journals_dc_Ladan-MozesS08, DBLP_conf_ppopp_YangM16}. The basket queue approach lies in the middle, where a failed \CAS{} in an enqueue operation implies concurrency with other enqueue operations. Therefore, the items of all these operations do not need to be ordered, and instead, they are stored in a basket where the items can be dequeued in any order~\cite{basketqueue2007}. A recent \CAS{} implementation from hardware transactional has been proposed to overcome this bottleneck, which exhibits better performance than the usual \CAS~\cite{scalingconcurrent2020}.

In Section~\ref{section:relaxed-concurrent}, examples of First-In-First-Out (FIFO) queues that applied relaxations were provided~\cite{DBLP_conf_cf_HaasLHPSKS13, DBLP_conf_pact_KirschLP13, DBLP_conf_ica3pp_KirschPRS12}. Following the work of Castañeda, Rajsbaum, and Raynal~\cite{DBLP_conf_opodis_CastanedaRR20} about the multiplicity relaxation, Johnen et al.~\cite{DBLP_conf_opodis_JohnenKM22}, propose a wait-free FIFO queue with multiple enqueuers and multiple dequeuers that if the semantics of the queue can be relaxed, by allowing concurrent \emph{dequeue} operations (multiplicity relaxation), they can achieve \(O(\log{n})\) worst-case complexity for \emph{enqueue} and \emph{dequeue} operations.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../../main"
%%% End:
