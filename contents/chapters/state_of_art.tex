\chapter{\label{chapter:2_State_of_art}State of the Art}

From the end of World War II until the 1990s, most computing was conducted on single-core processors. Operating systems utilized schedulers and other techniques to simulate concurrency. In 2001, the first multi-core processor was created by IBM~\cite{ibmIBM100Power}. This innovation enables two processors to collaborate and operate at high bandwidths (for its time), benefiting from significant on-chip memories and high-speed buses. Over time, processors have been equipped with more cores. It's essential to remember Moore's law, which suggests that the number of transistors in the same space continues to increase yearly. However, this results in smaller electronic components and circuits, which cannot be made faster without overheating. As a result, many industries are adopting ``multi-core'' architectures. In this setup, several processors communicate through shared memory, using hardware caches and RAM. This allows for more effective computing by utilizing parallelism, where the processors work together on a single task~\cite{DBLP_books_daglib_0020056}.

Implementing multiprocessors has brought about a significant shift in our software development approach. We can run multiple tasks in parallel in these multiprocessors and build more sophisticated tasks that use multiprocessors in parallel, executing several sub-tasks. We must exploit this parallelism to boost our efficiency. Usually, we can use techniques from parallel, distributed, or concurrent computing, which are related but distinct fields. Programming for multiprocessors can be complex due to the inherent asynchronicity of modern computer systems. This project centers on concurrent computing and explores the shift from traditional to more flexible approaches (relaxed concurrent computing). It takes a theoretical approach but with practical applications in mind.

\section{\label{section:classic-concurrent}Classic Concurrent Computing}

In the past, sequential computing was the standard method for performing computations before the emergency of concurrent computing. Based on the contributions of  Turing~\cite{DBLP_journals_x_Turing37} and Von Neumann~\cite{DBLP_journals_annals_Neumann93}, sequential computing involves executing instructions one after the other using a processor. This can be represented by the concept of \textit{objects} being modified by a \textit{process} through \textit{atomic operations}. The relationship between operations and objects can be defined in terms of \textit{preconditions} and \textit{postconditions}, which define the state of an object before and after applying the operation (since now, we will refer ``\textit{operation}'' as ``\textit{method}'' as in the context of \textit{Object-Oriented Programming}). This is similar to an API documentation\footnote{(Application Program Interface)}, which usually explains the object's state before invoking a method of the object and the result of calling the operation, which can be a particular value or throw an exception. This style of documentation is known as ``\textit{sequential specification}''.
However, expressing the relationship between objects and methods fails when several processes share such objects. The problem is that if many processes can invoke an object's operation concurrently, what invocation is first? What is the state after the execution of these overlapping invocations? Does it make sense to talk about operation order?

In concurrent systems, three consistency models are usually utilized as a correctness condition: \textit{Serializability}, \textit{Sequential Consistency}, and \textit{Linearizability}. The concept of Serializability was initially explained by Papadimitriou~\cite{DBLP_journals_jacm_Papadimitriou79b}. Sequential Consistency was introduced by Lamport~\cite{lamport1979how}. Lastly, Linearizability was introduced by Herlihy and Wing~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}.
Serializability in concurrent computing guarantees the correctness and isolation of transactions in a multi-user database or concurrent system. It ensures that when executing a set of transactions concurrently, the final result is equivalent to executing them one after another without overlap, mimicking a serial execution order. This helps maintain consistency and prevents errors in the system. 
Sequential Consistency mandates that the shared variable operations in a concurrent system appear to be executed sequentially, consistent with program order. Linearizability is a stricter condition that guarantees Sequential Consistency and ensures that the global order of operations includes a specific point in time (i.e., linearization point) for each operation. This ensures that each operation appears to take effect atomically at some point between its invocation and response. Linearizability refines the concept of Sequential Consistency by imposing a stricter requirement on the sequence of methods. This ensures that the system's observed behavior aligns with a valid sequential execution of the methods. Therefore, while Sequential Consistency allows for multiple valid orders of operations as long as they respect program order, Linearizability enforces a stricter condition by requiring operations to appear as if they occurred instantaneously at some specific point between invocation and response. 

In a concurrent multi-process system, a progress condition outlines the assurance of progress made by processes. It sets specific conditions that ensure processes in the system will keep advancing toward completing their tasks. Progress conditions are partitioned into \textit{blocking} and \textit{non-blocking}. There are two blocking progress conditions that rely on lock-based synchronization: Deadlock-freedom and starvation-freedom. Deadlock-freedom guarantees that processes will not deadlock and at least one process will make progress. This means that a process acquiring a lock, such process will release it, in other words, a process trying to acquire the lock eventually succeeds.



There are three \textit{non-blocking} progress conditions: \textit{Obstruction-Free}~\cite{DBLP_conf_icdcs_HerlihyLM03}, \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90} and \textit{Wait-Free}~\cite{DBLP_journals_toplas_Herlihy91}. 


\textit{Sequential Consistency} and \textit{Linearizability} are \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}. This ensures that their progress condition guarantees that \textit{some} method invocation finishes in a finite number of steps. \textit{Wait-free} progress condition~\cite{DBLP_journals_toplas_Herlihy91} is stronger than lock-free, where \textit{every} method invocation finishes its execution in a finite number of steps. When using lock-free methods, the system as a whole will make progress, but it does not guarantee that any specific thread will make progress. This is because lock freedom ensures \textit{minimal progress}. On the other hand, wait-freedom ensures the \textit{maximal progress}: any process that continues to take steps will make progress. Obstruction-free only guarantees progress if no other processes actively interfere with the process making progress~\cite{DBLP_conf_icdcs_HerlihyLM03}.


Attiya et al. ~\cite{DBLP_conf_popl_AttiyaGHKMV11} addresses the fundamental limitation in concurrent algorithms, arguing that the necessity of synchronization mechanisms is intrinsic and cannot be eliminated without incurring significant costs. 


\section{\label{section:relaxed-concurrent}Relaxed Concurrent Computing}
\section{\label{section:work-stealing}Work-Stealing}
\section{\label{section:data-structures}Data-Structures}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% TeX-parse-self: t
%%% TeX-auto-save: t
%%% End:
