\chapter{\label{chapter:2_State_of_art}State of the Art}

From the end of World War II until the 1990s, most computing was conducted on single-core processors. Operating systems utilized schedulers and other techniques to simulate concurrency. In 2001, IBM created the first multi-core processor~\cite{ibmIBM100Power}. This innovation enables two processors to collaborate and operate at high bandwidths (for its time), benefiting from significant on-chip memories and high-speed buses. Over time, processor manufacturers equip processors with more cores. It's essential to remember Moore's law, which suggests that the number of transistors in the same space continues to increase yearly. However, this results in smaller electronic components and circuits, which cannot be made faster without overheating. As a result, many industries are adopting ``multi-core'' architectures. In this setup, several processors communicate through shared memory, using hardware caches and RAM. This allows for more effective computing by utilizing parallelism, where the processors work together on a single task~\cite{DBLP_books_daglib_0020056}.

Implementing multiprocessors has brought about a significant shift in our software development approach. We can run multiple tasks in parallel in these multiprocessors and build more sophisticated tasks that use multiprocessors in parallel, executing several sub-tasks. We must exploit this parallelism to boost our efficiency. Usually, we can use techniques from parallel, distributed, or concurrent computing, which are related but distinct fields. Programming for multiprocessors can be complex due to the inherent asynchronicity of modern computer systems. This project centers on concurrent computing and explores the shift from traditional to more flexible approaches (relaxed concurrent computing). It takes a theoretical approach but with practical applications in mind.

\section{\label{section:classic-concurrent}Classic Concurrent Computing}

Since the beginning of electronic computing, sequential computing has been the standard method for performing computations before the emergency of concurrent computing. Based on the contributions of  Turing~\cite{DBLP_journals_x_Turing37} and Von Neumann~\cite{DBLP_journals_annals_Neumann93}, sequential computing involves executing instructions one after the other using a processor. This can be represented by the analogy of \textit{objects} being modified by a \textit{process} through \textit{atomic operations}. The relationship between operations and objects can be defined in terms of \textit{preconditions} and \textit{postconditions}, which represent the state of an object before and after applying the operation\footnote{Since now, we will refer ``\textit{operation}'' as ``\textit{method}'' as in the context of \textit{Object-Oriented Programming}.}. This is similar to an API documentation\footnote{(Application Program Interface)}, which usually explains the object's state before invoking a method of the object and the result of calling the operation, which can be a particular value or throw an exception. This style of documentation is known as ``\textit{sequential specification}''.
However, expressing the relationship between objects and methods fails when several processes share such objects. The problem is that if many processes can invoke an object's operation concurrently, what invocation is first? What is the state after the execution of these overlapping invocations? Does it make sense to talk about operation order?

In concurrent systems, three consistency models are usually utilized as a correctness condition: \textit{Serializability}, \textit{Sequential Consistency}, and \textit{Linearizability}. The concept of Serializability was initially explained by Papadimitriou~\cite{DBLP_journals_jacm_Papadimitriou79b}. Lamport introduced the notion of Sequential Consistency~\cite{lamport1979how}. Herlihy and Wing introduced the idea of Linearizability~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}.
Serializability in concurrent computing guarantees the correctness and isolation of transactions in a multi-user database or concurrent system. It ensures that when executing a set of transactions concurrently, the final result is equivalent to running them one after another without overlap, mimicking a serial execution order. This helps maintain consistency and prevents errors in the system. 
Sequential Consistency requires shared variable operations in concurrent systems to appear executed sequentially according to program order. Linearizability is a stricter condition that guarantees Sequential Consistency and ensures that the global order of operations includes a specific point in time (i.e., linearization point) for each operation. This ensures that each operation appears to take effect atomically at some point between its invocation and response. Linearizability refines the concept of Sequential Consistency by imposing a stricter requirement on the sequence of methods. This ensures that the system's observed behavior aligns with a valid sequential execution of the methods. Therefore, while Sequential Consistency allows for multiple valid orders of operations as long as they respect program order, Linearizability enforces a stricter condition by requiring operations to appear as if they occurred instantaneously at some specific point between invocation and response. 

In a concurrent multi-process system, a progress condition outlines the assurance of process progress. It sets specific requirements that ensure processes in the system will keep advancing toward completing their tasks. Progress conditions are partitioned into \textit{blocking} and \textit{non-blocking}. Two blocking progress conditions rely on lock-based synchronization: Deadlock-freedom and starvation-freedom~\cite{DBLP_books_daglib_0020056}. \textit{Deadlock-freedom} guarantees that processes will not deadlock and at least one process will make progress; this means that a process acquiring a lock will release it; in other words, a process trying to acquire the lock eventually succeeds. \textit{Starvation-freedom} ensures that every thread progresses as long as no other thread holds the lock.

On the other hand, there are three \textit{non-blocking} progress conditions: \textit{Obstruction-Free}~\cite{DBLP_conf_icdcs_HerlihyLM03}, \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90} and \textit{Wait-Free}~\cite{DBLP_journals_toplas_Herlihy91}. 
\textit{Lock-free} progress condition ensures that \textit{some} method invocation finishes in a finite number of steps. \textit{Wait-free} progress condition~\cite{DBLP_journals_toplas_Herlihy91} is stronger than lock-free, where \textit{every} method invocation finishes its execution in a finite number of steps. When using lock-free methods, the system as a whole will make progress, but it does not guarantee that any specific thread will make progress. This is because lock freedom ensures \textit{minimal progress}. On the other hand, wait-freedom ensures the \textit{maximal progress}: any process that continues to take steps will make progress. Obstruction-free~\cite{DBLP_conf_icdcs_HerlihyLM03} only guarantees progress if no other processes actively interfere with the process making progress. \textit{Sequential Consistency} and \textit{Linearizability} are \textit{Lock-Free}~\cite{DBLP_journals_toplas_HerlihyW90DBLP_journals_toplas_HerlihyW90}.

Consistency models and progress conditions are properties of the concurrent objects that show how they should behave and how they make progress. However, we still need other properties that tell us how powerful the methods are for solving synchronization problems. Herlihy introduced the notion of \textit{consensus number}~\cite{DBLP_journals_toplas_Herlihy91} as a measure of the computational power of concurrent objects. The \textit{consensus number} of a concurrent object is the maximum number of processes that can solve an elementary synchronization problem known as \textit{consensus} using concurrent objects, which are often called \textit{synchronization primitives}. Herlihy demonstrates that there exists an infinite hierarchy of synchronization primitives. No primitive at any given level can be used for a wait-free or lock-free implementation of any primitive at higher levels~\cite{DBLP_journals_toplas_Herlihy91}, i.e., in a system of \(n\) or more concurrent processes, it is impossible to implement a wait-free or lock-free object with consensus number \(n\) using objects with a lower consensus number.

Implementing efficient and correct concurrent algorithms is known to be a difficult problem.
To address the problem, currently, multiprocessors provide synchronization instructions that can be expressed as \textit{Read-Modify-Write} (RMW) operations\footnote{e.g., Compare\&Swap or Test\&Set.}, with high coordination power (measured through the consensus number~\cite{DBLP_journals_toplas_Herlihy91}), which are in principle slower than simple \textit{Read}/\textit{Write} instructions\footnote{In practice, an uncontended \textit{Read-Modify-Write} instruction can be faster than contended \textit{Read}/\textit{Write} instructions due to contention.}. In addition, certain programs may utilize Read-After-Write synchronization patterns that rely on the flag principle (see, for example,~\cite {DBLP_books_daglib_0020056}). This involves writing to a shared variable and then reading another variable. To ensure proper implementation of this synchronization pattern on multi-core architectures, a \textit{memory fence} (referred to as a \textit{barrier}) should be explicitly added to prevent the compiler or architecture from rearranging Read and Write instructions. It has been demonstrated that building concurrent implementations of classic and ubiquitous specifications\footnote{Such as sets, queues, stacks, and mutual exclusion.} in the standard asynchronous shared memory model must use \textit{Read-After-Write} synchronization patterns or atomic \textit{Read-Modify-Write} instructions. Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} addresses the fundamental limitation in concurrent algorithms, arguing that the necessity of synchronization mechanisms is intrinsic and cannot be eliminated without incurring significant costs. Ellen et al.~\cite{DBLP_journals_siamcomp_EllenHS12} show that shared data structures are often inherently sequential and cannot be easily parallelized. Attiya et al.~\cite{DBLP_journals_jacm_AttiyaGHK09} explores the advantages and drawbacks of obstruction-free implementations over other synchronization methods. These implementations can avoid the scalability and fault-tolerance problems that arise from traditional locking-based techniques, which can become a bottleneck in highly concurrent systems. Obstruction-free implementations can perform well without step contention but have high worst-case complexity.

\section{\label{section:relaxed-concurrent}Relaxed Concurrent Computing}

The impossibility of eliminating expensive synchronization in classic and ubiquitous specifications, shown in the work of Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11}, makes us ask: \textit{``Is it possible to circumvent this impossibility result in any way?''} Two possible ways to circumvent this result are: (1) consider relaxed semantics for the algorithms and (2) make extra assumptions about the model. 

Software development has become more challenging with the widespread adoption of multi-core processors as the standard computing platform. It is critical to optimize the use of all available computer resources, including multiple cores, memory, and storage, for efficient performance. Most programs need data structures, and with all these new multi-core computing platforms, concurrent data structures are required for implementing distributed, parallel, and concurrent programs. Designing concurrent data structures is a challenging task. The challenge arises when trying to enhance performance while maintaining correctness. As we strive to improve performance, ensuring the algorithm's correctness becomes increasingly more complex~\cite{DBLP_journals_cacm_Shavit11}. It has been mentioned to improve scalability, traditional data structures must be relaxed. This often involves relaxing safety and liveness conditions\footnote{Often called \textit{correctness} and \textit{progress} respectively.}. By relaxing the ordering guarantees of queues and stacks, performance and scalability can be significantly increased. There are many examples of natural relaxations that demonstrate this~\cite{DBLP_journals_cacm_Shavit11}. In the work of Shavit and Taubenfeld~\cite{DBLP_journals_dc_ShavitT16}, it is pointed out that relaxing the semantics of traditional data structures might be beneficial to reduce synchronization requirements and improve scalability: ``There is a trade-off between synchronization and the ability of an implementation to scale performance with the number of processors. Amdahlâ€™s law implies that even a small fraction of inherently sequential code limits scaling. Using semantically weaker data structures may help reduce the synchronization requirements and hence improve scalability for multi-core systems.''
Two types of relaxation are used: (1) relaxing the sequential specification of traditional data structures and (2) relaxing the requirements for correctness conditions. 

An example of the first case is the \(k-FIFO\) queue presented in the work of Kirsch et al. ~\cite{DBLP_conf_pact_KirschLP13, DBLP_conf_ica3pp_KirschPRS12}, where the sequential specification requirement was relaxed. The elements of this queue can be dequeued out-of-order up to a constant \(k \ge 0\) (called k-Out-of-Order). Hezinger et al.~\cite{DBLP_conf_popl_HenzingerKPSS13} address such redefinition of data structure semantics. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to sequential specification: the k-relaxed sequential contains all sequences over the alphabet within distance k from the original specification. This semantic specification defines the distance in terms of data structures~\cite{DBLP_conf_popl_HenzingerKPSS13}. Shavit and Taubenfeld conducted an analysis of relaxed queues, stacks, and multisets. They examine whether the relaxation of these data structures' semantics can result in more simple and scalable implementations. The authors evaluate these relaxations from a perspective of computability~\cite{DBLP_journals_dc_ShavitT16}. Also, in the work of Henzinger et al~\cite{DBLP_conf_popl_HenzingerKPSS13}, in addition to the definition of k-Out-of-Order, they define the k-Stuttering and the k-Lateness. Concurrent data structures can employ a relaxation scheme known as K-Stuttering, which allows for a certain amount of repetition or stuttering in operation execution. This relaxation enables an operation to be repeated up to \(k\) times before being considered a failure. While this can increase the performance of the data structure by reducing the need for synchronization, it may also compromise its correctness to some extent. The concept of K-lateness involves measuring the duration that an item remains on a stack without removal. This metric is determined by counting the number of pop operations that have occurred (\(k\)) since the item was last the youngest element added to the stack. K-lateness is particularly useful in the context of k-stuttering relaxation for concurrent data structures, as it helps to identify which items can be removed from the stack without violating the relaxation constraints.


In the second case, based on the research conducted by Afek et al.~\cite{DBLP_conf_opodis_AfekKY10}, the concept of quasi-linearizability is a way to quantify limited non-determinism. An object implementation is considered quasi-linearizable if each execution of the implementation is at a bounded ``distance'' way from some linear execution of the object. This definition is more flexible than linearizability and can improve the performance and scalability of concurrent object implementations. To illustrate, quasi-linearizability can be seen as a middle ground between linearizability and weaker consistency models. 

Another relaxation based on quantitative relaxation~\cite{DBLP_conf_cf_HaasLHPSKS13, DBLP_conf_popl_HenzingerKPSS13}. In the work of Talmage and Welch~\cite{DBLP_journals_algorithms_TalmageW18}, it is shown that Linearizability and the three data type relaxations studied in~\cite{DBLP_conf_popl_HenzingerKPSS13}, k-Out-of-Order, k-Lateness, and k-Stuttering, can also be defined as consistency conditions. In~\cite{DBLP_conf_concur_HaasHHKLPSSV16}, it is defined the notion of \textit{local linearizability}, which is a relaxed consistency condition that is applied to container data structures like pools, queues, and stacks.  The notion of distributional linearizability~\cite{DBLP_conf_spaa_Alistarh0KLN18} captures randomized relaxations. This formalism is applied to MultiQueues~\cite{DBLP_conf_spaa_RihaniSD15}, a family of concurrent data structures implementing relaxed concurrent priority queues.

\section{\label{section:work-stealing}Work-Stealing}
\section{\label{section:data-structures}Data-Structures}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main.tex"
%%% TeX-parse-self: t
%%% TeX-auto-save: t
%%% End:
