\chapter{\label{chapter:1_Introduction}Introduction}

In these days, it is very common to hear about new processors that increase the number of cores in each one. Tasks like gaming, data processing, rendering animation, and video editions are becoming more natural in the day-to-day. These tasks take advantage of the new processors and their multi-core architectures. It is worth mentioning that these multi-core processors are already present in laptops, smartphones, PCs, tablets, smart TVs, game consoles, multiple IoT\footnote{Internet of the Things.} devices, smartwatches, and even devices like keyboards!\footnote{My current keyboard is already a tiny computing device; it has a small screen on which small applications can be displayed and some additional controls like a knob to use such applications. Regarding the keyboard's processor specification, the provider mentioned that the processor follows a multi-core architecture without specifying which one.} No matter if we are specialized programmers like those who work in embedded systems or we are working on backend software or developing games, it is really important to design and code algorithms that take advantage of these multi-core architectures.

However, concurrent computing is one of the most challenging topics in computer science. This is because we are used to thinking in a sequence of steps. It is not easy to imagine multiple things happening simultaneously and randomly intermixing. When we program sequentially, it is easy to see that things occur in the same order every time, making it deterministic. However, concurrency introduces non-determinism since processes run independently, meaning things do not necessarily happen in the same order. As a result, all kind of unforeseen interactions can occur. When building concurrent algorithms, some things must be taken into account in the right order to avoid undesirable behaviors in concurrent executions. For example, we can consider the following reasons why concurrent programming can be challenging:

\begin{itemize}
  \item \textbf{Complexity}: Execute concurrent algorithms involve running multiple tasks simultaneously. Such executions can result in complex interactions and interdependencies between different parts of the program. Managing and coordinating all these interactions in a synchronized manner can be quite challenging.
  \item \textbf{Race Conditions}: A race condition occurs when the expected outcome or state of a shared variable relies on a specific sequence of events that are beyond the program's design. Usually, this problem can result in errors, unpredictable behavior, or bugs that are challenging to replicate.
.  \item \textbf{Synchronization}: To ensure that multiple processes can safely access shared resources, synchronization mechanisms such as locks, semaphores, barriers, or even concurrency primitives provided by processor architectures must be used. However, correctly managing these mechanisms can be challenging, as improper use can cause problems such as data corruption or performance issues. Therefore, it is essential to implement these mechanisms correctly to prevent such issues.
  \item \textbf{Performance}: Usually, we think that using multiple cores in parallel should improve the performance of a concurrent program. However, using shared resources and some factors like load balancing, synchronization, and unnecessary parallelization can degrade the performance of concurrent programs. We must carefully design and develop concurrent programs to achieve optimal performance.
  \item \textbf{Deadlocks}: A deadlock happens when two o more processes are waiting for each other to release resources. This results in a state where no process can make any progress. Deadlocks are usually challenging and hard identify and resolve, specially in complex systems
  \item \textbf{Scalability}: We want that our concurrent programs scale well as the number of processors or cores increases. However, ensure that concurrent programs improve when the number of processors increase requires careful design and optimization.
  \item \textbf{Learning Curve}: Additional concepts to sequential programming, like threads, processes, concurrency primitives, linearizability, and sequential consistency can require a significant learning curve.
  \item \textbf{Debugging and Testing}: The concurrent programs' nondeterministic nature makes them difficult to debug. Order-of-events-dependent bugs are also challenging to reproduce and diagnose. Testing such programs can be time-consuming and complex.

\end{itemize}

Several techniques have been developed to manage multiple processes accessing shared resources simultaneously and deal with the reasons mentioned above. These techniques include locks, semaphores, barriers, and primitives such as \RMW operations, which differ in their level of granularity. Using these techniques, synchronization patterns have been designed to handle situations where data is read after being written by multiple processes, known as \RAW patterns, which rely on the flag principle~\cite{DBLP_books_daglib_0020056}.


\section{\label{section:Motivation}Motivation}

Usually, to implement concurrent algorithms in the standard asynchronous shared memory model, we must use \RAW synchronization patterns or atomic \RMW instructions (e.g., \CAS or \TAS). As previously mentioned, \RAW patterns rely on the flag principle~\cite{DBLP_books_daglib_0020056}. Under this principle, when multiple processes write to a shared variable and then read from another variable, \textit{memory fences} (also known as \textit{barriers}) are necessary to prevent reordering of reads and writes by the processor or compiler. When implementing an algorithm that uses such synchronization patterns in modern multi-core architectures, the use of memory fences is crucial to ensure proper execution. However, it is well-known that the use of fences are highly costly, while \RMW instructions, with high coordination power (it can be formally measured through the \textit{consensus number} formalism~\cite{DBLP_journals_toplas_Herlihy91}), are in principle slower than the simple \R/\W instructions. In practice, contention might be the dominant factor, namely, an uncontended \RMW instruction can be faster than contended \R/\W instructions.

The work of Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} has shown that it is impossible to eliminate expensive synchronization in classic and ubiquitous algorithm specifications. This leads us to question \textit{if it is possible to bypass this impossibility result in any way}. There are two possible ways to circumvent this result: (1) consider relaxed semantics for the algorithms and (2) make additional assumptions about the model. Considering the reasons why concurrent programming can be challenging, we are interested in studying how to design and develop concurrent algorithms that can deal with all (or at least the majority) reasons shown previously using relaxed semantics. In particular, we want to explore the shift from traditional to more flexible concurrent computing approaches to circumvent the impossibility result mentioned previously. Additionally, we want to design modular and simple concurrent algorithms that can use distinct solutions (from classic synchronization methods to relaxed and flexible solutions) as if they were Lego pieces and study when relaxations can be useful in practical settings.

\section{\label{section:Objectives}Objectives And Contribution}

We are interested in the following theoretical questions:

\begin{enumerate}
  \item Are there useful relaxations that admit solutions using only synchronization mechanisms that are among the simplest ones?
  \item Is it possible to build modular concurrent algorithms that use relaxed solutions and are good enough to compete with classic algorithms in the state-of-the-art?
\end{enumerate}

As a first step, we explore the problem of the work-stealing in Chapter~\ref{chapter:4_work-stealing}, seeking for \R/\W wait-free, and fence-free solutions in the standard asynchronous shared memory model. Fence-free means that the algorithm's correctness does not require any specific instruction ordering beyond what is implied by data dependence. The combination of the three requirement, \R/\W based, wait-freedom, and fence-freedom, greatly restricts the structure of possible solutions. Every operation can only execute the \R instruction in a set of reads followed by the \W instruction in a set of writes, whose written values depend on the reads; in both cases, reads and writes instructions can be executed in any order. Despite the simplicity of the possible solution, we show that non-trivial and useful objects can be implemented.

We first consider work-stealing with multiplicity~\cite{DBLP_journals_dc_CastanedaRR23}, a relaxation in which every tasks is taken by \textit{at least} one \Take/\Steal operation, and, differently from idempotent work-stealing~\cite{maged.vechev.2009}, if several operations take on a task, they must be \textit{pairwise concurrent}. Therefore, no more than the number of processes in the system can take the same task. We study the case where tasks are inserted/extracted in FIFO order. We present a \R/\W wait-free algorithm for work-stealing with multiplicity, whose \Put operation is fence-free and \Take and \Steal operations are devoid of \RAW synchronization patterns. The step complexity  of \Put is constant, while \Take and \Steal have logarithmic steap complexity. Simplicity is a notable quality of the algorithm. It based on a single instance of \MaxReg object~\cite{DBLP_journals_jacm_AspnesAC12,DBLP_journals_siamcomp_JayantiTT00}, showing that work-stealing with multiplicity reduces to \MaxReg.

Then, we study a variant of multiplicity in which \Take/\Steal operations extracting the same task \emph{need not be concurrent}. However, each process extracts any task \emph{at most once} and hence the relaxed behavior \emph{is not allowed to happen} in sequential executions.  This relaxation is called work-stealing with \emph{weak multiplicity}. We present an algorithm inspired by our first solution, which uses only \R/\W instructions, is \emph{fence-free}, and all its operations are wait-free. Furthermore, each operation has constant step complexity. To our knowledge, this is the first algorithm for a relaxation of work-stealing with all these properties. The algorithm is obtained by reducing work-stealing with weak multiplicity to \RangeMaxReg, a relaxation of \MaxReg proposed here.

%Also, we show that each of the proposed algorithms can be easily modified so that every task is extracted by a bounded number of operations, more specifically, by at most one \Steal operation. In the modified algorithms, \Put and \Take remain the same, and a single \SWAP instruction is added to \Steal. Also, the algorithms can be modified to have multiplicity on demand: a task can be taken by multiple operations only if indicated. These variants can be used in contexts where repeatable work is not always allowed. Formal specifications and correctness proofs are provided using the \emph{linearizability}~\cite{DBLP_journals_toplas_HerlihyW90} and \emph{set-linearizable} correctness formalisms~\cite{DBLP_journals_jpdc_AttiyaCH18, DBLP_conf_podc_Neiger94}.  Intuitively, set-linearizability is a generalization of linearizability in which several concurrent operations are allowed to be linearized at the same linearization point.

%Work-stealing with multiplicity and idempotent work-stealing are closely related but different. We observe that the idempotent work-stealing algorithms have some corner cases allowing an unbounded number of \Steal operations to extract the same task. This observation implies that idempotent work-stealing algorithms do not solve work-stealing with multiplicity or weak multiplicity. Therefore, relaxation and algorithms proposed here provide stronger guarantees than idempotent work-stealing algorithms without the need for heavy synchronization mechanisms.



This thesis gathers results from a conference paper in 35th International Simposium on Distributed Computing, DISC 2021~\cite{DBLP_conf_wdag_CastanedaP21}, a journal paper in the Journal of Parallel and Distributed Computing~\cite{DBLP_journals_jpdc_CastanedaP24} and a preprint work published in ArXiv~\cite{arxiv_2205_06323}. Part of this thesis is a continuation of such preprint
\section{\label{section:Organization}Structure Of This Thesis}
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../../main"
%%% End:
