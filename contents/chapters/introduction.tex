\chapter{\label{chapter:1_Introduction}Introduction}

In these days, it is very common to hear about new processors that increase the number of cores in each one. Tasks like gaming, data processing, rendering animation, and video editions are becoming more natural daily. These tasks take advantage of the new processors and their multi-core architectures. It is worth mentioning that these multi-core processors are already present in laptops, smartphones, PCs, tablets, smart TVs, game consoles, multiple IoT\footnote{Internet of the Things.} devices, smartwatches, and even devices like keyboards!\footnote{My current keyboard is already a tiny computing device; it has a small screen on which small applications can be displayed and some additional controls like a knob to use such applications. Regarding the keyboard's processor specification, the provider mentioned that the processor follows a multi-core architecture without specifying which one.} No matter if we are specialized programmers like those who work in embedded systems or are working on back-end software or developing games, it is really important to design and code algorithms that take advantage of these multi-core architectures.

However, concurrent computing is one of the most challenging topics in computer science. This is because we are used to thinking in a sequence of steps. It is not easy to imagine multiple things happening simultaneously and randomly intermixing. When we program sequentially, it is easy to see that things occur in the same order every time, making it deterministic. However, concurrency introduces non-determinism since processes run independently, meaning things do not necessarily happen in the same order. As a result, all kinds of unforeseen interactions can occur. When building concurrent algorithms, some things must be considered properly to avoid undesirable behaviors in concurrent executions. For example, we can consider the following reasons why concurrent programming can be challenging:

\begin{itemize}
  \item \textbf{Complexity}: Executing concurrent algorithms involves running multiple tasks simultaneously. Such executions can result in complex interactions and interdependencies between different program parts. Managing and coordinating all these interactions in a synchronized manner can be quite challenging.
  \item \textbf{Race Conditions}: A race condition occurs when the expected outcome or state of a shared variable relies on a specific sequence of events that are beyond the program's design. Usually, this problem can result in errors, unpredictable behavior, or bugs that are challenging to replicate.
  \item \textbf{Synchronization}: To ensure that multiple processes can safely access shared resources, synchronization mechanisms such as locks, semaphores, barriers, or even concurrency primitives provided by processor architectures must be used. However, correctly managing these mechanisms can be challenging, as improper use can cause problems such as data corruption or performance issues. Therefore, it is essential to implement these mechanisms correctly to prevent such issues.
  \item \textbf{Performance}: Usually, we think that using multiple cores in parallel should improve the performance of a concurrent program. However, using shared resources and some factors like load balancing, synchronization, and unnecessary parallelization can degrade the performance of concurrent programs. We must carefully design and develop concurrent programs to achieve optimal performance.
  \item \textbf{Deadlocks}: A deadlock happens when two or more processes are waiting for each other to release resources. This results in a state where no process can make any progress. Deadlocks are usually challenging and complicated to identify and resolve, especially in complex systems
  \item \textbf{Scalability}: We want our concurrent programs to scale well as the number of processors or cores increases. However, ensuring that concurrent programs improve when the number of processors increases requires careful design and optimization.
  \item \textbf{Learning Curve}: Additional concepts concerning concurrent programming, like threads, processes, concurrency primitives, linearizability, and sequential consistency, can require a significant learning curve.
  \item \textbf{Debugging and Testing}: The concurrent programs' nondeterministic nature makes them difficult to debug. Order-of-events-dependent bugs are also challenging to reproduce and diagnose. Testing such programs can be time-consuming and complex.

\end{itemize}

Several techniques have been developed to manage multiple processes that access shared resources simultaneously and address the abovementioned reasons. These techniques include locks, semaphores, barriers, and primitives such as \RMW operations, which differ in their level of granularity. Using these techniques, synchronization patterns have been designed to handle situations where data is read after being written by multiple processes, known as \RAW patterns, which rely on the flag principle~\cite{DBLP_books_daglib_0020056}.


\section{\label{section:Motivation}Motivation}

Usually, to implement concurrent algorithms in the standard asynchronous shared memory model, we must use \RAW synchronization patterns or atomic \RMW instructions (e.g., \CAS or \TAS). As previously mentioned, \RAW patterns rely on the flag principle~\cite{DBLP_books_daglib_0020056}. Under this principle, when multiple processes write to a shared variable and then read from another variable, \textit{memory fences} (also known as \textit{barriers}) are necessary to prevent reordering of reads and writes by the processor or compiler. When implementing an algorithm that uses such synchronization patterns in modern multi-core architectures, using memory fences is crucial to ensure proper execution. However, it is well-known that the use of fences is highly costly, while \RMW instructions, with high coordination power (it can be formally measured through the \textit{consensus number} formalism~\cite{DBLP_journals_toplas_Herlihy91}), are in principle slower than the simple \R/\W instructions. In practice, contention might be the dominant factor; an uncontended \RMW instruction can be faster than contended \R/\W instructions.

The work of Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} has shown that it is impossible to eliminate expensive synchronization in classic and ubiquitous algorithm specifications. This leads us to question \textit{if it is possible to bypass this impossibility result in any way}. There are two possible ways to circumvent this result: (1) consider relaxed semantics for the algorithms and (2) make additional assumptions about the model. Considering the reasons why concurrent programming can be challenging, we are interested in studying how to design and develop concurrent algorithms that can deal with all (or at least the majority) reasons shown previously using relaxed semantics. In particular, we want to explore the shift from traditional to more flexible concurrent computing approaches to circumvent the impossibility result mentioned previously. Additionally, we want to design modular and simple concurrent algorithms that can use distinct solutions (from classic synchronization methods to relaxed and flexible solutions) as if they were Lego pieces and study when relaxations could be useful in practical settings.

\section{\label{section:Objectives}Objectives And Contribution}

We are interested in the following theoretical questions:

\begin{enumerate}
  \item Are there useful relaxations that admit solutions using only synchronization mechanisms that are among the simplest ones?
  \item Is it possible to build modular concurrent algorithms that use relaxed solutions and are good enough to compete with classic algorithms in the state-of-the-art?
\end{enumerate}

As a first step, we explore the problem of the work-stealing in Chapter~\ref{chapter:4_work-stealing}, seeking for \R/\W wait-free and fence-free solutions in the standard asynchronous shared memory model. Work-stealing is a popular technique for efficient task parallelization of irregular workloads by implementing dynamic load balancing. Fence-free means that the algorithm's correctness does not require any specific instruction ordering beyond what is implied by data dependence. The combination of the three requirements, \R/\W based, wait-freedom, and fence-freedom, dramatically restricts the structure of possible solutions. Every operation can only execute the \R instruction in a set of reads followed by the \W instruction in a set of writes, whose written values depend on the reads; in both cases, reads and writes instructions can be executed in any order. Despite the simplicity of the possible solution, we show that non-trivial and useful objects can be implemented.

We first consider work-stealing with multiplicity~\cite{DBLP_journals_dc_CastanedaRR23}, a relaxation in which every task is taken by \textit{at least} one \Take/\Steal operation, and, differently from idempotent work-stealing~\cite{maged.vechev.2009}, if several operations take on a task, they must be \textit{pairwise concurrent}. Therefore, no more than the number of processes in the system can take the same task. We study the case where tasks are inserted/extracted in FIFO order. We present a \R/\W wait-free algorithm for work-stealing with multiplicity, whose \Put operation is fence-free and \Take and \Steal operations are devoid of \RAW synchronization patterns. The step complexity of \Put is constant, while \Take and \Steal have logarithmic step complexity. Simplicity is a notable quality of the algorithm. It is based on a single instance of \MaxReg object~\cite{DBLP_journals_jacm_AspnesAC12,DBLP_journals_siamcomp_JayantiTT00}, showing that work-stealing with multiplicity reduces to \MaxReg.

Then, we study a variant of multiplicity in which \Take/\Steal operations extracting the same task \emph{need not be concurrent}. However, each process extracts any task \emph{at most once} and hence the relaxed behavior \emph{is not allowed to happen} in sequential executions.  This relaxation is called work-stealing with \emph{weak multiplicity}. We present an algorithm inspired by our first solution, which uses only \R/\W instructions, is \emph{fence-free}, and all its operations are wait-free. Furthermore, each operation has constant step complexity. To our knowledge, this is the first algorithm for a relaxation of work-stealing with all these properties. The algorithm is obtained by reducing work-stealing with weak multiplicity to \RangeMaxReg, a relaxation of \MaxReg proposed here.

%Also, we show that each of the proposed algorithms can be easily modified so that every task is extracted by a bounded number of operations, more specifically, by at most one \Steal operation. In the modified algorithms, \Put and \Take remain the same, and a single \SWAP instruction is added to \Steal. Also, the algorithms can be modified to have multiplicity on demand: a task can be taken by multiple operations only if indicated. These variants can be used in contexts where repeatable work is not always allowed. Formal specifications and correctness proofs are provided using the \emph{linearizability}~\cite{DBLP_journals_toplas_HerlihyW90} and \emph{set-linearizable} correctness formalisms~\cite{DBLP_journals_jpdc_AttiyaCH18, DBLP_conf_podc_Neiger94}.  Intuitively, set-linearizability is a generalization of linearizability in which several concurrent operations are allowed to be linearized at the same linearization point.

%Work-stealing with multiplicity and idempotent work-stealing are closely related but different. We observe that the idempotent work-stealing algorithms have some corner cases allowing an unbounded number of \Steal operations to extract the same task. This observation implies that idempotent work-stealing algorithms do not solve work-stealing with multiplicity or weak multiplicity. Therefore, relaxation and algorithms proposed here provide stronger guarantees than idempotent work-stealing algorithms without the need for heavy synchronization mechanisms.


We continue our study by addressing the second question presented at the beginning of this section. We adopt a modular approach to building concurrent algorithms to tackle this question. Specifically, we focus on the problem of \textit{multi-producer, multi-consumer concurrent FIFO queues}. Our modular approach for FIFO queue models the queue as a pair of objects to manage the \textit{head} and the \textit{tail}, along with a set of container objects to store the items inserted into the queue. To deal with concurrency during insertions/extractions, we consider the idea of \textit{baskets}~\cite{basketqueue2007} as the containers to store the items. Initially, baskets were considered as a way to reduce queue's \CAS contention in a variant of the Michael-Scott queue~\cite{DBLP_conf_podc_MichaelS96}, being defined implicitly. More recently, the basket concept basket was explicitly described as an abstract data type~\cite{scalingconcurrent2020}; nevertheless, in this work, we propose a basket specification that provides stronger guarantees and allows different basket implementations to continue with the modular design. We provide two distinct implementations for the baskets, the first one that follows an approach similar to that of the LCRQ algorithm~\cite{ppopp2013x86queues}, while the second implementation is reminiscent of locally linearizable generic data structure implementations of~\cite{DBLP_conf_concur_HaasHHKLPSSV16}.

 In the case of the objects to manage \textit{head} and \textit{tail}, we propose a novel object we call \llic, which resembles the well-known instruction \llsc, and, in a similar fashion to the baskets, it can be implemented of different manners. We even propose a solution that implements only \R/\W instructions instead of more sophisticated \RMW instructions to continue tackling the first question of this section. Another implementation of this type of object is based on \CAS instruction.

 We complement our results by performing an experimental evaluation for both case studies, i.e., for work-stealing and the modular basket queues. In the first experimental evaluation, we compare our work-stealing algorithms to the standard Cilk THE~\cite{DBLP_conf_pldi_FrigoLR98}, Chase-Lev~\cite{circular.work.stealing}, and idempotent work-stealing~\cite{maged.vechev.2009}. The algorithms were evaluated using three different benchmarks: (1) zero cost experiment, (2) parallel spanning tree, and (3) parallel SAT. While the work associated with each task is minimal in the first two benchmarks, the work associated with tasks is considerable in the third one. In the first two benchmarks, some of our algorithms exhibit similar and sometimes better performance than idempotent work-stealing algorithms, which outperform Cilk THE and Chase-Lev. However, in the third benchmark, no significant difference exists between all algorithms, either relaxed or not.

Similarly, we compare our modular queue algorithms to the Wait-Free queue by Yang and Mellor-Crummey~\cite{DBLP_conf_ppopp_YangM16}, the Lock-Free LCRQ queue by Morrison and Afek~\cite{ppopp2013x86queues}, the Lock-Free queue by Michael and Scott~\cite{DBLP_conf_podc_MichaelS96}, the Lock-Free queue by Ramalhete and Correia~\cite{Ramalhete_Correia_MPMC_2016} and the Lock-Free queue by Ostrovsky and Morrison~\cite{scalingconcurrent2020}, which use the idea of basket as well as us. The algorithms were evaluated using two benchmarks: (1) inner experiments and (2) outer experiments. In the first benchmark, we evaluate distinct combinations of \LL/\IC objects and baskets in the modular queue and compare their performance. The results show that the combination of \CAS-based \LL/\IC object with the basket implementation that follows a similar approach to that of LCRQ performed better than the other combinations. In the second benchmark, the previous combination had a better performance with respect to the queue of Ostrovsky and Morrison and the classic queue of Michael and Scott but it is outperformed by the fastest queues known in state-of-the-art (LCRQ, Yang-Mellor Crummey's queues).
% We continue our study proposing a multi-producer multi-consumer modular queue in Chapter~\ref{chapter:5_modular-basket-queues}, trying to answer the second question presented at the beginning of this section. We want to take a modular approach to building concurrent queues as if they were Legos.

This thesis gathers results from a conference paper in 35th International Symposium on Distributed Computing, DISC 2021~\cite{DBLP_conf_wdag_CastanedaP21}, a journal paper in the Journal of Parallel and Distributed Computing~\cite{DBLP_journals_jpdc_CastanedaP24} and a preprint work published in ArXiv~\cite{arxiv_2205_06323}. Part of this thesis is a continuation of such a preprint.
\section{\label{section:Organization}Structure Of This Thesis}

The rest of this thesis is structured as follows. In Chapter~\ref{chapter:2_State_of_art}, we discuss the state of the art concerning concurrent computing, relaxed concurrent computing, the problem of work-stealing, and concurrent queues. Chapter~\ref{chapter:3_preliminaries} presents the model of computation used for this work, the linearizability and set-linearizability formalisms, a background of hardware fundamentals where is discussed the use of memory fences and some architectures like TSO and x86, the relationship between consistency models in programming languages and the hardware, and finishing with the statistical methodology used for the experiments. Chapter~\ref{chapter:4_work-stealing} addresses the problem of work-stealing and presents the wait-free, fence-free, \R/\W algorithms to solve this problem. Chapter~\ref{chapter:5_modular-basket-queues} describes the design for the modular baskets queue and the distinct algorithms of the modules for this queue. Chapter~\ref{chapter:6_Results} presents the experimental evaluations and the results of both case studies. Chapter~\ref{sec:experiment-conclussions} closes this work, presenting the final discussion about the two case studies analyzed in this thesis and future research.
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../../main"
%%% End:
