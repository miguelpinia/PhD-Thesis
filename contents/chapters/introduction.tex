\chapter{\label{chapter:1_Introduction}Introduction}

In these days, it is very common to hear about new processors that increase the number of cores in each one. Tasks like gaming, data processing, rendering animation, and video editions are becoming more natural in the day-to-day. These tasks take advantage of the new processors and their multi-core architectures. It is worth mentioning that these multi-core processors are already present in laptops, smartphones, PCs, tablets, smart TVs, game consoles, multiple IoT\footnote{Internet of the Things.} devices, smartwatches, and even devices like keyboards!\footnote{My current keyboard is already a tiny computing device; it has a small screen on which small applications can be displayed and some additional controls like a knob to use such applications. Regarding the keyboard's processor specification, the provider mentioned that the processor follows a multi-core architecture without specifying which one.} No matter if we are specialized programmers like those who work in embedded systems or we are working on backend software or developing games, it is really important to design and code algorithms that take advantage of these multi-core architectures.

However, concurrent computing is one of the most challenging topics in computer science. This is because we are used to thinking in a sequence of steps. It is not easy to imagine multiple things happening simultaneously and randomly intermixing. When we program sequentially, it is easy to see that things occur in the same order every time, making it deterministic. However, concurrency introduces non-determinism since processes run independently, meaning things do not necessarily happen in the same order. As a result, all kind of unforeseen interactions can occur. When building concurrent algorithms, some things must be taken into account in the right order to avoid undesirable behaviors in concurrent executions. For example, we can consider the following reasons why concurrent programming can be challenging:

\begin{itemize}
  \item \textbf{Complexity}: Execute concurrent algorithms involve running multiple tasks simultaneously. Such executions can result in complex interactions and interdependencies between different parts of the program. Managing and coordinating all these interactions in a synchronized manner can be quite challenging.
  \item \textbf{Race Conditions}: A race condition occurs when the expected outcome or state of a shared variable relies on a specific sequence of events that are beyond the program's design. Usually, this problem can result in errors, unpredictable behavior, or bugs that are challenging to replicate.
.  \item \textbf{Synchronization}: To ensure that multiple processes can safely access shared resources, synchronization mechanisms such as locks, semaphores, barriers, or even concurrency primitives provided by processor architectures must be used. However, correctly managing these mechanisms can be challenging, as improper use can cause problems such as data corruption or performance issues. Therefore, it is essential to implement these mechanisms correctly to prevent such issues.
  \item \textbf{Performance}: Usually, we think that using multiple cores in parallel should improve the performance of a concurrent program. However, using shared resources and some factors like load balancing, synchronization, and unnecessary parallelization can degrade the performance of concurrent programs. We must carefully design and develop concurrent programs to achieve optimal performance.
  \item \textbf{Deadlocks}: A deadlock happens when two o more processes are waiting for each other to release resources. This results in a state where no process can make any progress. Deadlocks are usually challenging and hard identify and resolve, specially in complex systems
  \item \textbf{Scalability}: We want that our concurrent programs scale well as the number of processors or cores increases. However, ensure that concurrent programs improve when the number of processors increase requires careful design and optimization.
  \item \textbf{Learning Curve}: Additional concepts to sequential programming, like threads, processes, concurrency primitives, linearizability, and sequential consistency can require a significant learning curve.
  \item \textbf{Debugging and Testing}: The concurrent programs' nondeterministic nature makes them difficult to debug. Order-of-events-dependent bugs are also challenging to reproduce and diagnose. Testing such programs can be time-consuming and complex.

\end{itemize}

Several techniques have been developed to manage multiple processes accessing shared resources simultaneously and deal with the reasons mentioned above. These techniques include locks, semaphores, barriers, and primitives such as \RMW operations, which differ in their level of granularity. Using these techniques, various synchronization patterns have been designed to handle situations where data is read after being written by multiple processes, known as \RAW patterns, which rely on the flag principle~\cite{DBLP_books_daglib_0020056}.


\section{\label{section:Motivation}Motivation}

Considering the reasons why concurrent programming can be challenging, we are interested in studying how to design and develop concurrent algorithms that can deal with all (or at least the majority) reasons shown previously using relaxed semantics. In particular, we want to explore the shift from traditional to more flexible approaches in concurrent computing to programming concurrent algorithms. The work of Attiya et al.~\cite{DBLP_conf_popl_AttiyaGHKMV11} has shown that it is impossible to eliminate expensive synchronization in classic and ubiquitous algorithm specifications. Usually, to implement such algorithms in the standard asynchronous shared memory model, we must use \RAW synchronization patterns or atomic \RMW instructions (e.g., \CAS or \TAS).

As previously mentioned, \RAW patterns rely on the flag principle~\cite{DBLP_books_daglib_0020056}. Under this principle, when multiple processes write to a shared variable and then read from another variable, \textit{memory fences} (also known as \textit{barriers}) are necessary to prevent reordering of reads and writes by the processor or compiler. When implementing an algorithm that uses such synchronization patterns in modern multi-core architectures, the use of these memory fences is crucial to ensure proper execution. However, it is well-known that the use of fences are highly costly, while \RMW instructions, with high coordination power (it can be formally measured through the \textit{consensus number} formalism~\cite{DBLP_journals_toplas_Herlihy91}), are in principle slower than the simple \R/\W instructions. In practice, contention might be the dominant factor, namely, an uncontended \RMW instruction can be faster than contended \R/\W instructions.

\section{\label{section:Objectives}Objectives and Contribution}
\section{\label{section:Organization}Organization}
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../../main"
%%% End:
