\chapter{Case Of Study 2: Modular Basket Queues}

\section{Introduction}

Concurrent multi-producer/multi-consumer FIFO queues are fundamental shared data structures ubiquitous in all sorts of systems. Several concurrent queue shared-memory implementations have been proposed for over three decades. Despite these efforts, even state-of-the-art concurrent queue algorithms scale poorly; namely, as the number of cores grows, the latency of queue operations grows at least linearly on the number of cores.

One of the main reasons for the poor scalability is the high contention in the \RMW instructions, such as \CAS or \FAI, that manipulate the head and the tail~\cite{DBLP_conf_spaa_FatourouK11, DBLP_conf_ppopp_FatourouK12, basketqueue2007, DBLP_conf_ppopp_KoganP11, DBLP_journals_dc_Ladan-MozesS08, DBLP_conf_podc_MichaelS96, DBLP_journals_topc_Milman-SelaKLLP22, ppopp2013x86queues, scalingconcurrent2020, wfq-ppopp16}. The latency of any contended such instruction is linear in the number of contending cores since every instruction acquires exclusive ownership of its locationâ€™s cache line, and these acquisitions are serialized by the cache coherence protocol. The best-known queue implementations~\cite{ppopp2013x86queues, wfq-ppopp16} exploit the semantics of the \FAI instruction, that \emph{do not fail} and hence \emph{always make progress}.  In many queue implementations, a queue operation \emph{retries} a failed \CAS until it succeeds~\cite{DBLP_conf_spaa_FatourouK11, DBLP_conf_ppopp_FatourouK12, DBLP_conf_ppopp_KoganP11, DBLP_journals_dc_Ladan-MozesS08, DBLP_conf_podc_MichaelS96, DBLP_journals_topc_Milman-SelaKLLP22}.  An approach that lies in the middle is that of the \emph{baskets queue}~\cite{basketqueue2007}, where a failed \CAS in an enqueue operation implies concurrency with other enqueue operations, and hence, the items of all these operations do not need to be ordered. Instead, they are stored in a \emph{basket}, where the items can be dequeued in any order. To overcome this seemingly inherent bottleneck, it has been recently proposed a \CAS implementation from \emph{hardware transactional memory}, that exhibits better performance than the usual \CAS~\cite{scalingconcurrent2020}.

We observe that \RMW instructions are unnecessary to manipulate the head or tail consistently. We believe this observation may open the possibility of concurrent queue implementations with better scalability. Concretely, we present a \emph{modular} baskets queue algorithm based on a novel object that we call \emph{load-link/increment-conditional} (\LL/\IC) that suffices for manipulating the head and the tail of the queue.  \LL/\IC admits implementations that spread contention and use only simple \R/\W instructions.  \LL/\IC is similar to \LL/\SC, with the difference that \IC, if successful, only increments the current value of the linked register. The modular baskets queue stands for its simplicity, with a simple correctness proof.

\section{The modular basket queue}
\label{sec-basket-queue}

We consider the standard shared memory model~\cite{DBLP_journals_toplas_HerlihyW90} with $n \geq 2$ \emph{asynchronous} processes that communicate using \emph{atomic} instructions that modify the contents of the shared memory; the instructions range from simple \R and \W to more complex \RMW instructions such as \FAI and \CAS. For simplicity, the baskets queue algorithm is presented using an infinite shared array~\footnote{There are two common approaches to implement an infinite array, there are two common approaches. One is to use a circular dynamic array that can expand and shrink as needed. The other is to use a linked list where each node contains a finite-sized array. During execution, the list grows on demand, and each node is appended to the list using the Compare-And-Swap (\CAS) operation to maintain consistency.}. We consider the \emph{wait-free}~\cite{DBLP_journals_toplas_Herlihy91} and \emph{lock-free}~\cite{DBLP_conf_opodis_HerlihyS11} progress conditions, and \emph{linearizability}~\cite{DBLP_journals_toplas_HerlihyW90} as consistency condition.

\begin{figure}[H] \centering{ \fbox{
  
\begin{minipage}[t]{180mm} \footnotesize
\renewcommand{\baselinestretch}{2.5} \resetline
\begin{tabbing} aaaa\=aa\=aa\=aa\=aa\=aa\=aa\=\kill %~\\

{\bf Shared Variables:}\\
\> \(A[0, 1, \ldots] =\) \textit{infinite array of basket objects}\\
\>  $HEAD, TAIL =$  \textit{\LL/\IC objects initialized to 0} \\ \\

{\bf Operation} $\Enq(x)$: \\
\line{ABQ01} \> {\bf while \(\true\) do}\\
\line{ABQ02} \> \> $tail = TAIL.\LL()$ \\
\line{ABQ03} \> \> {\bf if $A[tail].\Put(x) == \ok$ then} \\
\line{ABQ04} \> \> \> $TAIL.\IC()$ \\
\line{ABQ05} \> \> \> \textbf{return} $\ok$\\
\line{ABQ06} \> \> {\bf endif} \\
\line{ABQ07} \> \> $TAIL.\IC()$ \\
\line{ABQ08} \> {\bf endwhile}\\
{\bf end \Enq} \\ \\

{\bf Operation} $\Deq()$: \\
\line{ABQ09} \> $head = HEAD.\LL()$ \\
\line{ABQ10} \> $tail = TAIL.\LL()$ \\
\line{ABQ11} \> {\bf while \(\true\) do}\\
\line{ABQ12} \> \> {\bf if $head < tail$ then}\\
\line{ABQ13} \> \> \> $x = A[head].\Take()$\\
\line{ABQ14} \> \> \> {\bf if $x \neq \closed$ then return $x$ endif}\\
\line{ABQ15} \> \> \> $HEAD.\IC()$ \\
\line{ABQ16} \> \> {\bf endif} \\
\line{ABQ17} \> \> $head' = HEAD.\LL()$ \\
\line{ABQ18} \> \> $tail' = TAIL.\LL()$\\
\line{ABQ19} \> \> {\bf if $head == head' == tail' == tail$ then return $\epty$ endif}\\
\line{ABQ20} \> \> $head = head'$ \\
\line{ABQ21} \> \> $tail = tail'$\\
\line{ABQ22} \> {\bf endwhile}\\
{\bf end \Deq}
\end{tabbing}
\end{minipage} }
\caption{\label{basket-queue}The modular baskets queue.} 
}
\end{figure}

The modular baskets queue appears in Algorithm~\ref{basket-queue}. It is based on two concurrent objects: baskets and \LL/\IC. Roughly speaking, the baskets store groups of enqueued items that can be taken dequeued in any order, while two \LL/\IC objects store the head and the tail of the queue.

A \emph{basket of capacity $K$} or \emph{$K$-basket}, is a data-structure that can hold up to $K$ items. The state of the basket is represented by a pair $(S, C)$ where $S$ is the set of items that can be added concurrently and $C$ is the number of items in the basket. A \emph{$K$-basket} is initialized to $(\emptyset, 0)$. The sequential specification of a \emph{$k$-basket} satisfies the following properties:

\begin{enumerate}
    \item $\Put(x)$. Non-deterministically can return \full (regardless of the state) or \ok. If $C = K$, then return \full, in another case do $S = S \cup \{x\}$, $C = C +1$ and return $\ok$.
    \item $\Take()$. If $S \neq \emptyset$, then do $S = S \setminus \{x\}$ and return $x$, for some $x \in S$, else do $C = K$ and return \closed.
\end{enumerate}

The baskets in the original baskets queue algorithm~\cite{basketqueue2007} were defined only \emph{implicitly}. Recently, baskets were explicitly defined in the work of Ostrovsky and Morrison~\cite{scalingconcurrent2020}. Our basket specification provides stronger guarantees; the main difference is the following: in the work of Ostrovsky and Morrison~\cite{scalingconcurrent2020}, a {\sf basket\_empty} operation can return either \true or \false if the basket is not empty, i.e., it allows false negatives. The \Take operation of our specification mixes the functionality of {\sf basket\_empty} and {\sf basket\_extract} as if it returns \closed, no item will ever be put or taken from the basket.

The specification of the object \LL/\IC satisfies the next two properties, where the state of the object is an integer $R$, initialized to 0, and assuming that any process invokes \IC only if it has invoked \LL before, then the specification for these operations is the following:

\begin{enumerate}
\item $\LL()$: Returns the current value in $R$.
\item $\IC()$: If $R$ has not been increment since the last \LL of the invoking process,
then do $R = R + 1$; in any case return \ok.
\end{enumerate}


\begin{theorem}
  In the Algorithm~\ref{basket-queue}, if the objects $HEAD$ and $TAIL$ of type \LL/\IC, are linearizable and wait-free, then the algorithm is a linearizable lock-free implementation of a concurrent queue.
\end{theorem}

\begin{proof}
  Since all shared objects are wait-free, every implementation step is completed. Note that every time a \Deq/\Enq operation completes a while loop (hence without returning), an \Enq (resp. a \Deq) operation successfully puts (resp. takes) an item in (resp. from) a basket. Thus, in an infinite execution, if a \Deq/\Enq operation takes infinitely many steps, infinitely many \Deq/\Enq operations terminate. Hence, the implementation is lock-free.

  We consider the aspect-oriented linearizability proof framework in~\cite{DBLP_conf_concur_HenzingerSV13} to prove that the algorithm is linearizable. Assuming that every item is enqueued at most once, it states that a queue implementation is linearizable if each of its finite executions is \emph{free} of four violations. We enumerate the violations and argue that every algorithm execution is free of them.

  VFresh: A \Deq operation returns an item not previously inserted by any \Enq operation.  \Deq operations return items once put in the baskets, and \Enq operations put items in the baskets. Thus, each execution is free of VFresh.

  VRepeat: Two \Deq operations return the item inserted by the same \Enq operation. The specification of the basket directly implies that every execution is free of VRepeat.

  VOrd: Two items are enqueued in a certain order, and a \Deq returns the later item before any \Deq of the earlier item starts.  \LL/\IC guarantees that if an \Enq operation enqueues an item, say $x$, and then a later \Enq operation enqueues another item, say $y$, then $x$ and $y$ are inserted in baskets $A[i]$ and $A[j]$, with $i < j$. Then, $x$ is dequeued first because \Deq operations scan $A$ in index-ascending order. Thus, every execution is free of VOrd.

  VWit: A \Deq operation returning \epty even though the queue is never logically empty during the execution of the \Deq operation. An item is logically in the queue if it is in a basket $A[i]$ and $i < TAIL$. When a \Deq operation returns \epty, there is a point in time where no basket in $A[0, 1, \hdots, TAIL-1]$ contains an item, and hence the queue is logically empty (it might, however, be the case that $A[TAIL]$ does contain an item at that moment). Hence, every execution is free of VWit.
\end{proof}

The algorithm's scalability depends on the scalability of the concrete implementations of \LL/\IC and the basket with which it is instantiated. We propose wait-free implementations of each of the objects.

\paragraph{\LL/\IC implementations.} Let $p$ denote the process that invokes an operation.

\paragraph{A CAS-based implementation.}
It uses a shared register $R$ initialized to 0.  \LL first reads $R$ and stores the value in a persistent variable $r_p$ of $p$, and then returns $r_p$.  \IC first reads $R$ and if that value is equal to $r_p$, then it performs $\CAS(R, r_p, r_p+1)$; in any it case returns \ok.

\begin{theorem}
The CAS-based \LL/\IC implementation just described is linearizable and wait-free.
\end{theorem}

\begin{proof}[Proof sketch.]
  The algorithm is wait-free. For the linearizability proof, consider any finite execution $E$ with no pending operations. We define the following linearization points. The linearization point of an \LL operation is when it reads $R$. If an \IC operation performs a \CAS, it is linearized at that step. Otherwise, it is linearized when it reads $R$. Let $S_t$ be the sequential execution induced by the first $t$ linearization points of $E$, reading its steps in index-ascending order. By induction on $t$, it can be shown that $S_T$ is a sequential execution of \LL/\IC, where $T$ is the number of operations in $E$. The main observation is that if there is a successful \CAS before the \CAS of an \IC operation of a process $p$, then the contents of $R$ are different from the value $p$ reads in its previous \LL operation.
\end{proof}

\paragraph{A \R/\W implementation.}
It uses a shared array $M$ with $n$ entries initialized to 0.  \LL first reads all entries of $M$ (in some order), stores the maximum value in a persistent variable $max_p$ of $p$, and then returns $max_p$.  \IC first reads all entries of $M$, and if the maximum among those values is equal to $max_p$, it performs $\W(M[p], max_p + 1)$; in any it case returns \ok.

\begin{theorem}
  The \R/\W-based \LL/\IC implementation just described is linearizable and wait-free.
\end{theorem}

\begin{proof}[Proof sketch]
The algorithm is wait-free. We next argue that each of its executions is linearizable.

Consider any finite execution of the algorithm with no pending operations. To simplify the argument, suppose that there is a \emph{fictitious} \IC operation that atomically writes 0 in all entries of $M$ at the beginning of the execution.

Each \IC operation is linearized at its last step. Thus, an \IC that writes is linearized at its \W step, and an \IC that does not write is linearized at its last \R step. Let $MAX$ be the maximum value in the shared array $M$ at the end of the execution. For every $R \in \{0,1, \hdots, MAX\}$, let $\IC_R$ be the \IC operation that writes $R$ for the first time in $M$.

We will linearize every \LL operation that returns the value $R \in \{0, 1, \hdots, MAX-1\}$ at one of its steps and argue that this step is between $\IC_R$ and $\IC_{R+1}$. This will induce a sequential execution that respects the real-time order and is a sequential execution of \LL/\IC, hence a linearization.

Let \op denote any \LL that returns $R \in \{0, 1, \hdots, MAX-1\}$ and let $e$ denote its \R step that reads $R$ for the first time. Observe that $IC_R$ has been linearized when $e$ happens in the execution. We have two cases:

\begin{enumerate}
\item If the shared memory $M$ does not contain a value $> R$ when $e$ occurs (hence no $IC_{R'}$ with $R' > R$ has been linearized when $e$ occurs), then \op is linearized at $e$.

\item If the shared memory $M$ does contain a value $> R$ when $e$ occurs, then \op is linearized as follows.  Let $M[j]$ be the entry read at step $e$. Note that this case can happen if and only if some entries in the range $M[0, \hdots, j-1]$ contain values $> R$ when $e$ happens (and hence some $IC_{R'}$ with $R' > R$ have been linearized when $e$ occurs). Moreover, it can be shown that the value $R+1$ is written in an entry in the range $M[0, \hdots, j-1]$ at some time between the invocation of \op and $e$. Let $i \in \{0, \hdots, j-1\}$ be the index of the entry where it is written $R+1$ for the first time. Then, \op is linearized right before $R+1$ is written in $M[i]$ (and hence before $\IC_{R+1}$).
\end{enumerate}

\end{proof}

\paragraph{A mixed implementation.}
It uses a shared array $M$ with $K < n$ entries initialized to 0.  \LL reads all entries of $M$ and stores the maximum value and index in persistent variables $max_p$ and $indmax_p$ of $p$, and returns $max_p$.  \IC non-deterministically picks an index $pos \in \{0,1,\hdots,K-1\} \setminus \{indmax_p\}$.  If $M[pos]$ contains a value $x$ less than $max_p+1$, then it performs $\CAS(M[pos], x, max_p+1)$; if the \CAS is successful, it returns \ok. Otherwise, it reads the value in $M[indmax_p]$, and if it is equal to $max_p$, then it performs $\CAS(M[indmax_p], max_p, max_p+1)$; in any it case returns~\ok.

\begin{theorem}
  The mixed implementation just described is linearizable and wait-free.
\end{theorem}

\begin{proof}[Proof sketch.]
  The algorithm is wait-free. The linearizability proof is nearly the same as the one in the previous theorem proof; the only difference is that each \IC operation is linearized at its last step, either a \CAS (successful or not) or a \R.
\end{proof}


\begin{figure} \centering{ \fbox{
  

  \begin{minipage}[t]{180mm} \footnotesize
\renewcommand{\baselinestretch}{2.5} \resetline
\begin{tabbing} aaaa\=aa\=aa\=aa\=aa\=aa\=aa\=\kill %~\\

{\bf Shared Variables:}\\
\> \(A[0, 1, \ldots, K-1] = [\bot, \bot, \ldots, \bot]\) \\
\> \(PUTS, TAKES = 0\) \\
\> \(STATE = \open\) \\ \\

{\bf Operation} $\Put(x)$: \\
\line{BBQ01} \> {\bf while \(\true\) do}\\
\line{BBQ02} \> \> \(state = \R(STATE)\) \\
\line{BBQ03} \> \> \(puts = \R(PUTS)\) \\
\line{BBQ04} \> \> {\bf if \(state == \closed\) or \(puts \geq K\) then return \full} \\
\line{BBQ05} \> \> {\bf else} \\
\line{BBQ06} \> \> \> \(puts = \FAI(PUTS)\) \\
\line{BBQ07} \> \> \> {\bf if \(puts \geq K\) then return \full} \\
\line{BBQ08} \> \> \> {\bf else if \(\SWAP(A[puts], x) == \bot\) then return \ok endif} \\
\line{BBQ09} \> \> {\bf endif} \\
\line{BBQ10} \> {\bf endwhile}\\
{\bf end \Put} \\ \\

{\bf Operation} $\Take()$: \\
\line{BBQ11} \> {\bf while \(\true\) do}\\
\line{BBQ12} \> \> \(state = \R(STATE)\) \\
\line{BBQ13} \> \> \(takes = \R(TAKES)\) \\
\line{BBQ14} \> \> {\bf if \(state == \closed\) or \(takes \geq K\) then return \closed} \\
\line{BBQ15} \> \> {\bf else} \\
\line{BBQ16} \> \> \> \(takes = \FAI(TAKES)\) \\
\line{BBQ17} \> \> \> {\bf if \(takes \geq K\) then} \\
\line{BBQ18} \> \> \> \> \(\W(STATE, \closed)\) \\
\line{BBQ19} \> \> \> \> \textbf{return} \closed \\
\line{BBQ20} \> \> \> {\bf else} \\
\line{BBQ21} \> \> \> \> \(x = \SWAP(A[puts], \top)\) \\
\line{BBQ22} \> \> \> \> {\bf if \(x \neq \bot\) then return \(x\) endif} \\
\line{BBQ23} \> \> \> {\bf endif} \\
\line{BBQ24} \> \> {\bf endif} \\
\line{BBQ25} \> {\bf endwhile}\\
{\bf end \Take}
\end{tabbing}
\end{minipage}}
\caption{\label{basket-1}$K$-basket from \FAI and \SWAP.}
}
\end{figure}

\begin{figure}[H]  \centering{ \fbox{
  
  \begin{minipage}[t]{180mm} \footnotesize
\renewcommand{\baselinestretch}{2.5} \resetline
\begin{tabbing} aaaa\=aa\=aa\=aa\=aa\=aa\=aa\=\kill %~\\

{\bf Shared Variables:}\\
\> \(A[0, 1, \ldots, n-1] = [\bot, \bot, \ldots, \bot]\) \\
\> \(STATE = \open\) \\
{\bf Persistent Local Variables of $p$:}\\
\> \(takes_p = \{0, 1, \hdots, n-1\}\)\\ \\

{\bf Operation} $\Put(x)$: \\
\line{CBQ01} \> {\bf if \(\R(STATE) == \closed\) then return \full} \\
\line{CBQ02} \> {\bf else if \(\R(A[p]) == \bot \) then  } \\
\line{CBQ03} \> \> {\bf if \(\CAS(A[p], \bot, x) \) then return \ok endif} \\
\line{CBQ04} \> {\bf endif} \\
\line{CBQ05} \> {\bf return \full}\\
{\bf end \Put} \\ \\

{\bf Function} $\compete(pos)$: \\
\line{CBQ06} \> \(x = \R(A[pos])\) \\
\line{CBQ07} \> {\bf if \(x == \top \) then return \(\top\)} \\
\line{CBQ08} \> {\bf else if \( \CAS(A[pos], x, \top)  \) then return \(x\)  } \\
\line{CBQ09} \> {\bf else return \(\bot\) endif} \\
{\bf end \compete} \\ \\

{\bf Operation} $\Take()$: \\
\line{CBQ10} \> {\bf while \(\true\) do}\\
\line{CBQ11} \> \> {\bf if \(\R(STATE) == \closed\) then return \closed} \\
\line{CBQ12} \> \> {\bf else} \\
\line{CBQ13} \> \> \> {\bf if \(p \in takes_p\) then \(pos = p\)} \\
\line{CBQ14} \> \> \> {\bf else} \(pos = \hbox{any element of } takes_p\) {\bf endif} \\
\line{CBQ15} \> \> \> \(takes_p =  takes_p \setminus \{pos\}\) \\
\line{CBQ16} \> \> \> {\bf if \(takes_p == \emptyset\) then \(\W(STATE, \closed)\) endif} \\
\line{CBQ17} \> \> \> \( x = \compete(pos) \) \\
\line{CBQ18} \> \> \> {\bf if \(x \neq \bot, \top \) then return \(x\)} \\
\line{CBQ19} \> \> \> {\bf else if \(x == \bot\) then} \\
\line{CBQ20} \> \> \> \> \( x = \compete(pos) \) \\
\line{CBQ21} \> \> \> \> {\bf if \(x \neq \bot, \top \) then return \(x\) endif} \\
\line{CBQ22} \> \> \>{\bf endif} \\
\line{CBQ23} \> \> {\bf endif} \\
\line{CBQ24} \> {\bf endwhile}\\
{\bf end \Take}
\end{tabbing}
\end{minipage}}
\caption{\label{basket-2}$n$-basket from \CAS. Let $p$ denote the invoking process.}
}
\end{figure}

\paragraph{Basket implementations.}
The basket implementations appear in Algorithms~\ref{basket-1} and~\ref{basket-2}. Both implementations have a shared array where the items are put and taken. A \Put operation tries to put its item in a location, while a \Take operation either takes an item from a location or marks it as "canceled."  The first implementation follows an approach similar to that of the LCRQ algorithm~\cite{ppopp2013x86queues}, while the second implementation is reminiscent of locally linearizable generic data structure implementations of~\cite{DBLP_conf_concur_HaasHHKLPSSV16}.

In the first implementation, the processes use \FAI to guarantee that at most two "opposite" operations "compete" for the same location in the shared array, which can be resolved with a \SWAP; the idea of this algorithm is similar to the approach in the LCRQ algorithm~\cite{ppopp2013x86queues}.

In the second implementation, each process has a dedicated location in the shared array where it tries to put its item when it invokes \Put. When a process invokes \Take, it first tries to take an item from its dedicated location. If it does not succeed, it randomly picks a non-previously-picked location, does the same, and repeats until it takes an item or all locations have been canceled. Since several operations might "compete" for the same location, \CAS is needed. This implementation is reminiscent of \emph{locally linearizable} generic data structure implementations of~\cite{DBLP_conf_concur_HaasHHKLPSSV16}.

\begin{theorem}
Algorithm~\ref{basket-1} is a wait-free linearizable implementation of a $K$-basket.
\end{theorem}

\begin{proof}[Proof sketch]
It is not hard to see that the algorithm is wait-free.

For the linearizability proof, given an entry $A[i]$, we will say that a \Put operation \emph{successfully puts} its item in $A[i]$ if it gets $\bot$ when it performs \SWAP on $A[i]$, and that a \Take operation \emph{successfully cancels} $A[i]$ if it gets $\bot$ when it performs \SWAP on $A[i]$, otherwise (i.e. it gets a value distinct from $\bot$), we say that the \Take operation \emph{successfully takes} an item from $A[i]$.

From the specification of \FAI, for every $A[i]$, at most one \Put operations tries to put its item in $A[i]$ successfully, and at most one \Take operation tries to either successfully cancel $A[i]$ or successfully take an item from $A[i]$. By the specification of \SWAP, if $A[i]$ is canceled, no \Put operation successfully puts an item in it, and no \Take operation successfully takes an item from it.

Given any execution of the algorithm, the operations are linearized as follows. A \Put operation that successfully puts its item is linearized at its last \FAI instruction before returning. A \Take operation that successfully takes an item from $A[i]$ is linearized right after the \Put operation that successfully puts its item in $A[i]$. A \Put that returns \full is linearized at its return step, and similarly, a \Take that returns \closed is linearized at its return step. Note that, in both cases, at that moment of the execution, every entry of $A$ has been or will be either canceled or a \Take operation has or will successfully take an item from it. It can be shown that these linearization points induce a valid linearization of the execution.
\end{proof}

\begin{theorem}
Algorithm~\ref{basket-2} is a wait-free linearizable implementation of an $n$-basket.
\end{theorem}

\begin{proof}[Proof sketch]
Clearly, \Put is wait-free. It is not difficult to see that \Take is wait-free too.

For the linearizability proof, given an entry $A[i]$, we will say that a \Put operation of a process $p$, \emph{successfully puts} its item in $A[p]$ if its \CAS is successful. A \Take operation \emph{successfully cancels} $A[i]$ if its $\CAS(A[i], x, \top)$ (in the \compete function) is successful, with $x$ being $\bot$; and it \emph{successfully takes} an item from $A[i]$ if its $\CAS(A[i], x, \top)$ (in the \compete function) is successful, with $x$ being distinct to $\bot$ and $\top$.

The linearizability proof is similar to the linearizability proof in the previous theorem, with the following main differences. (1) If a \Put operation returns \full, it can be the case that some of the other entries of $A$ will never be canceled or store an item; the response of the \Put operation is, however, correct because the sequential specification of $n$-basket allows \Put to return \full in any state of the object. (2) Several \Take operations might try to cancel the same entry $A[i]$ or successfully take an item from it; this is not a problem because the specification of \CAS guarantees that, at most, one succeeds.

Given any execution of the algorithm, the operations are linearized as follows. A \Put operation that successfully puts its item is linearized at its (successful) \CAS. A \Take operation that successfully takes an item from $A[i]$ is linearized right after the \Put operation that successfully puts its item in $A[i]$. A \Put that returns \full is linearized at its return step, and similarly, a \Take that returns \closed is linearized at its return step. Note that at the execution, a \Take that returns \closed, every entry of $A$ has been either canceled, or a \Take operation has successfully taken an item from it. It can be shown that these linearization points induce a valid linearization of the execution.
\end{proof}

\section{Coping with realistic assumptions}

Distinct from the single-producer multi-consumer queue presented in Chapter~\ref{chapter:4_work-stealing}, we are dealing with a multi-producer multi-consumer environment in this case. 

\begin{figure}[H] \centering{ \fbox{
  
\begin{minipage}[t]{180mm} \footnotesize
\renewcommand{\baselinestretch}{2.5} \resetline
\begin{tabbing} aaaa\=aa\=aa\=aa\=aa\=aa\=aa\=\kill %~\\

{\bf Additional Structures and Operations}\\
\> {\bf struct} \(Segment:\) \\
\>\> \(items[1, \ldots, N]\) \textit{array of basket objects of size N}\\
\>\> $HEAD, TAIL = $ \textit{\LL/\IC objects initialized to 0}\\
\>\> $next = $ \textit{Pointer to the next segment} \\
\> {\bf end struct}\\ \\

\> {\bf Operation} \(isFull(segment*)\) \\
\>\> {\bf return} $segment\rightarrow TAIL.\LL() \ge N$ \\
\> {\bf end Operation} \\ \\

\> {\bf Operation} \(isClosed(segment*)\) \\
\>\> {\bf return} $segment\rightarrow HEAD.\LL() \ge N$ \\
\> {\bf end Operation} \\ \\

{\bf Shared Variables:}\\
\> \(Head, Tail = \) \textit{Pointers to objects of type Segment, initially pointing to a sentinel object} \\ \\

{\bf Operation} $\Enq(x)$: \\
\line{BQ01} \> {\bf while \(\true\) do}\\
\line{BQ02} \>\> $lastTail = Tail$ \\
\line{BQ03} \>\> {\bf if $lastTail \ne Tail$ then continue; endif} \\
\line{BQ04} \>\> $lastNext = lastTail\rightarrow next$ \\
\line{BQ05} \>\> {\bf if \(lastNext \ne \bot\) then}\\
\line{BQ06} \>\>\> {\bf \(Tail.CAS(lastTail, lastNext)\); continue;} \\
\line{BQ07} \>\> {\bf endif}\\
\line{BQ08} \>\> {\bf \(ticket = lastTail\rightarrow TAIL.\LL()\)} \\
\line{BQ09} \>\> {\bf if \(isFull(lastTail)\) then} \\
\line{BQ10} \>\>\> {\bf \(newSegment = new\ Segment()\)} \\
\line{BQ11} \>\>\> {\bf \(newSegment\rightarrow{}items[0].put(val)\)};\\
\line{BQ12} \>\>\> {\bf \(newSegment\rightarrow{}TAIL.\IC()\)}\\
\line{BQ13} \>\>\> {\bf if \(lastTail\rightarrow{}next.CAS(\bot, newSegment)\) then}\\
\line{BQ14} \>\>\>\> \(Tail.CAS(lastTail, newSegment)\)\\
\line{BQ15} \>\>\>\> {\bf return \ok}\\
\line{BQ16} \>\>\> {\bf endif}\\
\line{BQ17} \>\>\> {\bf if \(lastTail\rightarrow{}items[ticket].put(x) == \ok\)} \\
\line{BQ18} \>\>\>\> \(lastTail\rightarrow{}TAIL.\IC()\) \\
\line{BQ19} \>\>\>\> {\bf return \ok} \\
\line{BQ20} \>\>\> {\bf endif} \\
\line{BQ21} \>\> {\bf endif}\\
\line{BQ22} \> {\bf endwhile}\\
{\bf end \Enq} \\ \\

\end{tabbing}
\end{minipage} }
\caption{\label{basket-queue-linked-list-enq}The modular baskets queue using linked-lists. Enqueue operation.} 
}
\end{figure}

\begin{figure}[H] \centering{ \fbox{
  
\begin{minipage}[t]{180mm} \footnotesize
\renewcommand{\baselinestretch}{2.5} \resetline
\begin{tabbing} aaaa\=aa\=aa\=aa\=aa\=aa\=aa\=\kill %~\\

{\bf Additional Structures and Operations}\\
\> {\bf struct} \(Segment:\) \\
\>\> \(items[1, \ldots, N]\) \textit{array of basket objects of size N}\\
\>\> $HEAD, TAIL = $ \textit{\LL/\IC objects initialized to 0}\\
\>\> $next = $ \textit{Pointer to the next segment} \\
\> {\bf end struct}\\ \\

\> {\bf Operation} \(isFull(segment*)\) \\
\>\> {\bf return} $segment\rightarrow TAIL.\LL() \ge N$ \\
\> {\bf end Operation} \\ \\

\> {\bf Operation} \(isClosed(segment*)\) \\
\>\> {\bf return} $segment\rightarrow HEAD.\LL() \ge N$ \\
\> {\bf end Operation} \\ \\

{\bf Shared Variables:}\\
\> \(Head, Tail = \) \textit{Pointers to objects of type Segment, initially pointing to a sentinel object} \\ \\

{\bf Operation} $\Deq()$: \\
\line{BQ23} \> {\bf while \(true\) do}\\
\line{BQ24} \>\> \(lastHead = Head\) \\
\line{BQ25} \>\> {\bf if \(lastHead \ne \bot\) then return \epty endif}\\
\line{BQ26} \>\> {\bf if \(lastHead \ne Head\) then continue endif}\\
\line{BQ27} \>\> {\bf if \(isClosed(lastHead)\) then}\\
\line{BQ28} \>\>\> {\bf \(next = lastHead->next\)} \\
\line{BQ29} \>\>\> \(Head.CAS(lastHead, next)\) \\
\>\>\> {\textit{\# Memory reclamation can be performed after the previous instruction}}\\
\line{BQ30} \> \> {\bf endif} \\
\line{BQ31} \>\> \(tailTicket = lastHead\rightarrow{}TAIL.\LL()\) \\
\line{BQ32} \>\> \(headTicket = lastHead\rightarrow{}HEAD.\LL()\) \\
\line{BQ33} \>\> {\bf while \(\neg isClosed(lastHead)\) do} \\
\line{BQ34} \>\>\> {\bf if \(headTicket < tailTicket\) then} \\
\line{BQ35} \>\>\>\> \(x = lastHead\rightarrow{}items[headTicket].\Take()\) \\
\line{BQ36} \>\>\>\> {\bf if \(x \ne \closed\) then return \(x\) endif} \\
\line{BQ37} \>\>\>\> \(lastHead\rightarrow{}HEAD.\IC()\) \\
\line{BQ38} \>\>\> {\bf endif} \\
\line{BQ39} \> \> $head' = HEAD.\LL()$ \\
\line{BQ40} \> \> $tail' = TAIL.\LL()$\\
\line{BQ41} \> \> {\bf if $head == head' == tail' == tail$ then return $\epty$ endif}\\
\line{BQ42} \> \> $headTicket = head'$ \\
\line{BQ43} \> \> $tailTicket = tail'$\\
\line{BQ44S} \> {\bf endwhile}\\
{\bf end \Deq}
\end{tabbing}
\end{minipage} }
\caption{\label{basket-queue-linked-list-deq}The modular baskets queue using linked-lists. Dequeue Operation} 
}
\end{figure}



\section{Experiments}

To evaluate the performance of our queue, we designed a set of experiments that allow us to know if it is competitive against other queues in the literature. We divide our experiments into two classes: \emph{inner experiments} \sout{(this name could change in the future)} and \emph{outer experiments}. We used the internal experiments to know which one of our implementations (LL/IC object and queue based on distinct variants of the LL/IC objects) has the best performance and throughput. Once the inner experiments and the best option are chosen, we evaluate the queue against queues in the state of the art. We compare our queue against the following:

\begin{enumerate}
\item Queue from Michael and Scott \cite{DBLP_conf_podc_MichaelS96}.
\item Queue from Yang and Mellor-Crummey \cite{DBLP_conf_ppopp_YangM16}.
\item We try to compare against the Queue from Ostrovsky and Morrison, but we are limited by hardware \cite{scalingconcurrent2020}. (Possible change).
\end{enumerate}



\subsection{Inner Experiments}
\label{sec:org4649774}

To evaluate the performance of our distinct variants for the LL/IC Object
and the queue, we perform the following experiments:

\begin{enumerate}
\item For an initial empty LL/IC Object, we perform a fixed number of calls to
LL/IC pairs. In each iteration, a thread executes a call to the LL method
followed by a call to the IC method. These calls are spread evenly among
a fixed number of threads, which work concurrently. Each thread performs
a random amount of "fake work" between both calls to avoid long-run
scenarios
\cite{DBLP_conf_ppopp_YangM16,DBLP_conf_podc_MichaelS96}. This fake
work is just an empty spin.
\item To test our concurrent queue, we perform two benchmarks. The first one is
to perform enqueue-dequeue pairs similar to the previous experiment. In
the second one, in each iteration, each thread decides randomly to
execute a dequeue or enqueue with the same probability. The total of
calls is partitioned evenly among all threads.
\end{enumerate}


\subsection{Update experiments}
\label{sec:org5027fac}

To update the experiments, we must understand what metrics
Let us compare the algorithms designed for LL/IC objects and Baskets.

Some benchmarks used to test concurrent queues are:

\begin{itemize}
\item enqueue - dequeue pairs:
\item 50\% enqueues
\end{itemize}

In both benchmarks, some work is added to avoid long-run scenarios. This
anomaly is described in \cite{DBLP_conf_podc_MichaelS96} and to avoid it, the
work added consists of spinning a small amount of time (6 \(\mu\)s) in an
empty loop. The idea behind this is to prevent long runs of queue operations
by the same process without this being interrupted, so this would display
an overly optimistic performance due to the lower cache miss rate.